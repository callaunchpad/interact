{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ipdb\n",
    "import h5py\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import utils.io as io\n",
    "from model.cnn_model import HOCNN\n",
    "from datasets import metadata\n",
    "from utils.vis_tool import vis_img\n",
    "from datasets.hico_constants import HicoConstants\n",
    "from datasets.hico_dataset import HicoDataset, collate_fn\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(21)\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "TRAIN_IMG_PATH = \"datasets/hico/images/train2015/\"\n",
    "TEST_IMG_PATH = \"datasets/hico/images/test2015/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup training device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('training on {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment v1_2020-11-03_17-35\n"
     ]
    }
   ],
   "source": [
    "# Define arguments\n",
    "batch_size = 32\n",
    "epochs = 80\n",
    "initial_lr = 0.00001\n",
    "\n",
    "feat_type = 'fc7'\n",
    "data_aug = False\n",
    "exp_ver = 'v1_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "save_dir = './checkpoints/hico'\n",
    "log_dir = './log/hico'\n",
    "save_every = 5\n",
    "print_batch_every = 100\n",
    "print_epoch_every = 1\n",
    "\n",
    "# set the cache size [0 means infinite]\n",
    "max_img_cache_size = 5000\n",
    "\n",
    "print('Running experiment ' + exp_ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fc7 feature...\n",
      "Using fc7 feature...\n",
      "set up dataset variable successfully\n",
      "set up dataloader successfully\n"
     ]
    }
   ],
   "source": [
    "# Define dataloaders\n",
    "data_const = HicoConstants(feat_type=feat_type)\n",
    "\n",
    "train_dataset = HicoDataset(data_const=data_const, subset='train', data_aug=data_aug)\n",
    "val_dataset = HicoDataset(data_const=data_const, subset='val', data_aug=False, test=True)\n",
    "dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "print('set up dataset variable successfully')\n",
    "\n",
    "train_dataloader = DataLoader(dataset=dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_dataloader = DataLoader(dataset=dataset['val'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "dataloader = {'train': train_dataloader, 'val': val_dataloader}\n",
    "print('set up dataloader successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = HOCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in this model is 49.470376 million\n"
     ]
    }
   ],
   "source": [
    "# Display parameter information\n",
    "parameter_num = 0\n",
    "for param in model.parameters():\n",
    "    parameter_num += param.numel()\n",
    "print(f'The number of parameters in this model is {parameter_num / 1e6} million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=0)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup visualization\n",
    "writer = SummaryWriter(log_dir=log_dir + '/' + exp_ver + '/' + 'epoch_train')\n",
    "io.mkdir_if_not_exists(os.path.join(save_dir, exp_ver, 'epoch_train'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyl02/.conda/envs/Interact/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "/home/jimmyl02/launchpad/interact/model/cnn_model.py:173: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(summed_results)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Epoch: 1/80 Batch: 100/952 Loss: 6.461737632751465 Accuracy: 6.25\n",
      "[train] Epoch: 1/80 Batch: 200/952 Loss: 5.453140735626221 Accuracy: 3.125\n",
      "[train] Epoch: 1/80 Batch: 300/952 Loss: 4.965389728546143 Accuracy: 3.125\n",
      "[train] Epoch: 1/80 Batch: 400/952 Loss: 5.457126617431641 Accuracy: 6.25\n",
      "[train] Epoch: 1/80 Batch: 500/952 Loss: 5.385848522186279 Accuracy: 0.0\n",
      "[train] Epoch: 1/80 Batch: 600/952 Loss: 5.058994293212891 Accuracy: 0.0\n",
      "[train] Epoch: 1/80 Batch: 700/952 Loss: 4.646044731140137 Accuracy: 12.5\n",
      "[train] Epoch: 1/80 Batch: 800/952 Loss: 4.414399147033691 Accuracy: 6.25\n",
      "[train] Epoch: 1/80 Batch: 900/952 Loss: 4.728868007659912 Accuracy: 15.625\n",
      "[train] Epoch: 1/80 Loss: 5.7592109216082426 Accuracy: 5.361534677816035 Execution time: 1173.434383392334\n",
      "[val] Epoch: 1/80 Batch: 100/238 Loss: 5.014231204986572 Accuracy: 6.25\n",
      "[val] Epoch: 1/80 Batch: 200/238 Loss: 4.594542026519775 Accuracy: 12.5\n",
      "[val] Epoch: 1/80 Loss: 4.8164817823069495 Accuracy: 8.985963531418077 Execution time: 437.20369386672974\n",
      "[train] Epoch: 2/80 Batch: 100/952 Loss: 4.7396697998046875 Accuracy: 6.25\n",
      "[train] Epoch: 2/80 Batch: 200/952 Loss: 4.648054122924805 Accuracy: 9.375\n",
      "[train] Epoch: 2/80 Batch: 300/952 Loss: 4.656371116638184 Accuracy: 6.25\n",
      "[train] Epoch: 2/80 Batch: 400/952 Loss: 4.690161228179932 Accuracy: 6.25\n",
      "[train] Epoch: 2/80 Batch: 500/952 Loss: 4.109127521514893 Accuracy: 15.625\n",
      "[train] Epoch: 2/80 Batch: 600/952 Loss: 4.567076206207275 Accuracy: 6.25\n",
      "[train] Epoch: 2/80 Batch: 700/952 Loss: 3.8817877769470215 Accuracy: 28.125\n",
      "[train] Epoch: 2/80 Batch: 800/952 Loss: 4.065709590911865 Accuracy: 25.0\n",
      "[train] Epoch: 2/80 Batch: 900/952 Loss: 4.692931652069092 Accuracy: 3.125\n",
      "[train] Epoch: 2/80 Loss: 4.628706009854331 Accuracy: 10.860796851942942 Execution time: 757.9557099342346\n",
      "[val] Epoch: 2/80 Batch: 100/238 Loss: 4.561065673828125 Accuracy: 18.75\n",
      "[val] Epoch: 2/80 Batch: 200/238 Loss: 4.462033748626709 Accuracy: 12.5\n",
      "[val] Epoch: 2/80 Loss: 4.524829961082902 Accuracy: 11.281647645284009 Execution time: 198.24478912353516\n",
      "[train] Epoch: 3/80 Batch: 100/952 Loss: 4.269584655761719 Accuracy: 6.25\n",
      "[train] Epoch: 3/80 Batch: 200/952 Loss: 4.407170295715332 Accuracy: 12.5\n",
      "[train] Epoch: 3/80 Batch: 300/952 Loss: 4.597343444824219 Accuracy: 6.25\n",
      "[train] Epoch: 3/80 Batch: 400/952 Loss: 4.358805179595947 Accuracy: 3.125\n",
      "[train] Epoch: 3/80 Batch: 500/952 Loss: 4.38261604309082 Accuracy: 15.625\n",
      "[train] Epoch: 3/80 Batch: 600/952 Loss: 4.324765205383301 Accuracy: 15.625\n",
      "[train] Epoch: 3/80 Batch: 700/952 Loss: 4.078707695007324 Accuracy: 12.5\n",
      "[train] Epoch: 3/80 Batch: 800/952 Loss: 4.663949489593506 Accuracy: 6.25\n",
      "[train] Epoch: 3/80 Batch: 900/952 Loss: 4.55815315246582 Accuracy: 21.875\n",
      "[train] Epoch: 3/80 Loss: 4.393209333243107 Accuracy: 12.985735366453516 Execution time: 790.0328540802002\n",
      "[val] Epoch: 3/80 Batch: 100/238 Loss: 4.318474292755127 Accuracy: 6.25\n",
      "[val] Epoch: 3/80 Batch: 200/238 Loss: 4.652687072753906 Accuracy: 9.375\n",
      "[val] Epoch: 3/80 Loss: 4.395929478917178 Accuracy: 13.157549521185885 Execution time: 193.54738426208496\n",
      "[train] Epoch: 4/80 Batch: 100/952 Loss: 3.884611129760742 Accuracy: 25.0\n",
      "[train] Epoch: 4/80 Batch: 200/952 Loss: 4.458207130432129 Accuracy: 3.125\n",
      "[train] Epoch: 4/80 Batch: 300/952 Loss: 4.1354660987854 Accuracy: 12.5\n",
      "[train] Epoch: 4/80 Batch: 400/952 Loss: 4.58089542388916 Accuracy: 12.5\n",
      "[train] Epoch: 4/80 Batch: 500/952 Loss: 4.047415733337402 Accuracy: 21.875\n",
      "[train] Epoch: 4/80 Batch: 600/952 Loss: 4.653493881225586 Accuracy: 6.25\n",
      "[train] Epoch: 4/80 Batch: 700/952 Loss: 4.813999176025391 Accuracy: 6.25\n",
      "[train] Epoch: 4/80 Batch: 800/952 Loss: 4.579567909240723 Accuracy: 9.375\n",
      "[train] Epoch: 4/80 Batch: 900/952 Loss: 4.005449295043945 Accuracy: 12.5\n",
      "[train] Epoch: 4/80 Loss: 4.223514863986972 Accuracy: 14.687653713723561 Execution time: 757.474916934967\n",
      "[val] Epoch: 4/80 Batch: 100/238 Loss: 4.367414474487305 Accuracy: 6.25\n",
      "[val] Epoch: 4/80 Batch: 200/238 Loss: 3.8431601524353027 Accuracy: 15.625\n",
      "[val] Epoch: 4/80 Loss: 4.301916769729048 Accuracy: 14.193886921159649 Execution time: 191.16838812828064\n",
      "[train] Epoch: 5/80 Batch: 100/952 Loss: 3.7367849349975586 Accuracy: 15.625\n",
      "[train] Epoch: 5/80 Batch: 200/952 Loss: 3.962812662124634 Accuracy: 9.375\n",
      "[train] Epoch: 5/80 Batch: 300/952 Loss: 3.9684245586395264 Accuracy: 21.875\n",
      "[train] Epoch: 5/80 Batch: 400/952 Loss: 4.468170642852783 Accuracy: 12.5\n",
      "[train] Epoch: 5/80 Batch: 500/952 Loss: 4.295217514038086 Accuracy: 21.875\n",
      "[train] Epoch: 5/80 Batch: 600/952 Loss: 4.649612903594971 Accuracy: 9.375\n",
      "[train] Epoch: 5/80 Batch: 700/952 Loss: 4.1564507484436035 Accuracy: 25.0\n",
      "[train] Epoch: 5/80 Batch: 800/952 Loss: 3.866129159927368 Accuracy: 25.0\n",
      "[train] Epoch: 5/80 Batch: 900/952 Loss: 4.055430889129639 Accuracy: 18.75\n",
      "[train] Epoch: 5/80 Loss: 4.090105417341967 Accuracy: 16.251844564682735 Execution time: 755.6806342601776\n",
      "[val] Epoch: 5/80 Batch: 100/238 Loss: 3.812737464904785 Accuracy: 9.375\n",
      "[val] Epoch: 5/80 Batch: 200/238 Loss: 4.418776988983154 Accuracy: 12.5\n",
      "[val] Epoch: 5/80 Loss: 4.2455801555338795 Accuracy: 14.495605404696313 Execution time: 194.89905095100403\n",
      "[train] Epoch: 6/80 Batch: 100/952 Loss: 4.090672492980957 Accuracy: 18.75\n",
      "[train] Epoch: 6/80 Batch: 200/952 Loss: 4.403911113739014 Accuracy: 21.875\n",
      "[train] Epoch: 6/80 Batch: 300/952 Loss: 3.0361530780792236 Accuracy: 28.125\n",
      "[train] Epoch: 6/80 Batch: 400/952 Loss: 3.611537456512451 Accuracy: 21.875\n",
      "[train] Epoch: 6/80 Batch: 500/952 Loss: 3.8202290534973145 Accuracy: 21.875\n",
      "[train] Epoch: 6/80 Batch: 600/952 Loss: 3.6134753227233887 Accuracy: 12.5\n",
      "[train] Epoch: 6/80 Batch: 700/952 Loss: 3.6631906032562256 Accuracy: 21.875\n",
      "[train] Epoch: 6/80 Batch: 800/952 Loss: 4.201655864715576 Accuracy: 9.375\n",
      "[train] Epoch: 6/80 Batch: 900/952 Loss: 3.798196792602539 Accuracy: 25.0\n",
      "[train] Epoch: 6/80 Loss: 3.955473645621977 Accuracy: 17.747171667486473 Execution time: 756.1608748435974\n",
      "[val] Epoch: 6/80 Batch: 100/238 Loss: 3.7709028720855713 Accuracy: 12.5\n",
      "[val] Epoch: 6/80 Batch: 200/238 Loss: 3.866844654083252 Accuracy: 25.0\n",
      "[val] Epoch: 6/80 Loss: 4.211924833527672 Accuracy: 15.40076085530631 Execution time: 192.18550443649292\n",
      "[train] Epoch: 7/80 Batch: 100/952 Loss: 5.096583366394043 Accuracy: 3.125\n",
      "[train] Epoch: 7/80 Batch: 200/952 Loss: 4.3744893074035645 Accuracy: 6.25\n",
      "[train] Epoch: 7/80 Batch: 300/952 Loss: 3.5289413928985596 Accuracy: 18.75\n",
      "[train] Epoch: 7/80 Batch: 400/952 Loss: 4.075531005859375 Accuracy: 21.875\n",
      "[train] Epoch: 7/80 Batch: 500/952 Loss: 3.57698655128479 Accuracy: 21.875\n",
      "[train] Epoch: 7/80 Batch: 600/952 Loss: 3.9199464321136475 Accuracy: 15.625\n",
      "[train] Epoch: 7/80 Batch: 700/952 Loss: 3.8165230751037598 Accuracy: 25.0\n",
      "[train] Epoch: 7/80 Batch: 800/952 Loss: 3.8948559761047363 Accuracy: 6.25\n",
      "[train] Epoch: 7/80 Batch: 900/952 Loss: 4.272004127502441 Accuracy: 12.5\n",
      "[train] Epoch: 7/80 Loss: 3.827341527355599 Accuracy: 19.822921790457453 Execution time: 767.590451002121\n",
      "[val] Epoch: 7/80 Batch: 100/238 Loss: 3.808875799179077 Accuracy: 25.0\n",
      "[val] Epoch: 7/80 Batch: 200/238 Loss: 4.157712459564209 Accuracy: 21.875\n",
      "[val] Epoch: 7/80 Loss: 4.184256362364578 Accuracy: 16.148497966679784 Execution time: 196.9810450077057\n",
      "[train] Epoch: 8/80 Batch: 100/952 Loss: 3.297614336013794 Accuracy: 34.375\n",
      "[train] Epoch: 8/80 Batch: 200/952 Loss: 3.700899362564087 Accuracy: 25.0\n",
      "[train] Epoch: 8/80 Batch: 300/952 Loss: 3.5514817237854004 Accuracy: 31.25\n",
      "[train] Epoch: 8/80 Batch: 400/952 Loss: 3.9189372062683105 Accuracy: 18.75\n",
      "[train] Epoch: 8/80 Batch: 500/952 Loss: 3.9272103309631348 Accuracy: 12.5\n",
      "[train] Epoch: 8/80 Batch: 600/952 Loss: 3.6871609687805176 Accuracy: 6.25\n",
      "[train] Epoch: 8/80 Batch: 700/952 Loss: 4.220020771026611 Accuracy: 9.375\n",
      "[train] Epoch: 8/80 Batch: 800/952 Loss: 4.120665073394775 Accuracy: 9.375\n",
      "[train] Epoch: 8/80 Batch: 900/952 Loss: 3.3526434898376465 Accuracy: 18.75\n",
      "[train] Epoch: 8/80 Loss: 3.6938035596255534 Accuracy: 21.285456632234794 Execution time: 785.8558211326599\n",
      "[val] Epoch: 8/80 Batch: 100/238 Loss: 4.060338973999023 Accuracy: 15.625\n",
      "[val] Epoch: 8/80 Batch: 200/238 Loss: 3.8915722370147705 Accuracy: 31.25\n",
      "[val] Epoch: 8/80 Loss: 4.157461487400896 Accuracy: 16.463334645152827 Execution time: 195.45005750656128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Epoch: 9/80 Batch: 100/952 Loss: 3.124918222427368 Accuracy: 18.75\n",
      "[train] Epoch: 9/80 Batch: 200/952 Loss: 3.582306385040283 Accuracy: 25.0\n",
      "[train] Epoch: 9/80 Batch: 300/952 Loss: 3.370269536972046 Accuracy: 25.0\n",
      "[train] Epoch: 9/80 Batch: 400/952 Loss: 3.2232918739318848 Accuracy: 28.125\n",
      "[train] Epoch: 9/80 Batch: 500/952 Loss: 2.864689826965332 Accuracy: 34.375\n",
      "[train] Epoch: 9/80 Batch: 600/952 Loss: 3.630892038345337 Accuracy: 28.125\n",
      "[train] Epoch: 9/80 Batch: 700/952 Loss: 3.6716930866241455 Accuracy: 18.75\n",
      "[train] Epoch: 9/80 Batch: 800/952 Loss: 5.0678839683532715 Accuracy: 6.25\n",
      "[train] Epoch: 9/80 Batch: 900/952 Loss: 3.3424129486083984 Accuracy: 28.125\n",
      "[train] Epoch: 9/80 Loss: 3.563912565072143 Accuracy: 23.21364158058698 Execution time: 748.080039024353\n",
      "[val] Epoch: 9/80 Batch: 100/238 Loss: 3.8234221935272217 Accuracy: 21.875\n",
      "[val] Epoch: 9/80 Batch: 200/238 Loss: 3.478285789489746 Accuracy: 28.125\n",
      "[val] Epoch: 9/80 Loss: 4.157655231786591 Accuracy: 16.97494424767152 Execution time: 192.02208638191223\n",
      "[train] Epoch: 10/80 Batch: 100/952 Loss: 3.414990186691284 Accuracy: 15.625\n",
      "[train] Epoch: 10/80 Batch: 200/952 Loss: 3.3050732612609863 Accuracy: 21.875\n",
      "[train] Epoch: 10/80 Batch: 300/952 Loss: 3.7325515747070312 Accuracy: 18.75\n",
      "[train] Epoch: 10/80 Batch: 400/952 Loss: 3.4380877017974854 Accuracy: 25.0\n",
      "[train] Epoch: 10/80 Batch: 500/952 Loss: 3.4354169368743896 Accuracy: 34.375\n",
      "[train] Epoch: 10/80 Batch: 600/952 Loss: 3.3113367557525635 Accuracy: 28.125\n",
      "[train] Epoch: 10/80 Batch: 700/952 Loss: 3.405421733856201 Accuracy: 28.125\n",
      "[train] Epoch: 10/80 Batch: 800/952 Loss: 4.238375186920166 Accuracy: 12.5\n",
      "[train] Epoch: 10/80 Batch: 900/952 Loss: 3.291116237640381 Accuracy: 28.125\n",
      "[train] Epoch: 10/80 Loss: 3.4396452548953818 Accuracy: 24.876209214625348 Execution time: 752.8466148376465\n",
      "[val] Epoch: 10/80 Batch: 100/238 Loss: 4.140807151794434 Accuracy: 12.5\n",
      "[val] Epoch: 10/80 Batch: 200/238 Loss: 4.0375847816467285 Accuracy: 18.75\n",
      "[val] Epoch: 10/80 Loss: 4.157449915702146 Accuracy: 17.184835366653548 Execution time: 192.29713463783264\n",
      "[train] Epoch: 11/80 Batch: 100/952 Loss: 3.6471543312072754 Accuracy: 25.0\n",
      "[train] Epoch: 11/80 Batch: 200/952 Loss: 3.1947085857391357 Accuracy: 25.0\n",
      "[train] Epoch: 11/80 Batch: 300/952 Loss: 3.119023323059082 Accuracy: 28.125\n",
      "[train] Epoch: 11/80 Batch: 400/952 Loss: 3.086883068084717 Accuracy: 31.25\n",
      "[train] Epoch: 11/80 Batch: 500/952 Loss: 3.3489530086517334 Accuracy: 31.25\n",
      "[train] Epoch: 11/80 Batch: 600/952 Loss: 3.536287307739258 Accuracy: 15.625\n",
      "[train] Epoch: 11/80 Batch: 700/952 Loss: 3.5415637493133545 Accuracy: 18.75\n",
      "[train] Epoch: 11/80 Batch: 800/952 Loss: 4.04063606262207 Accuracy: 18.75\n",
      "[train] Epoch: 11/80 Batch: 900/952 Loss: 3.5553243160247803 Accuracy: 21.875\n",
      "[train] Epoch: 11/80 Loss: 3.3112185912125227 Accuracy: 26.827348745696014 Execution time: 760.8923897743225\n",
      "[val] Epoch: 11/80 Batch: 100/238 Loss: 4.210273742675781 Accuracy: 21.875\n",
      "[val] Epoch: 11/80 Batch: 200/238 Loss: 4.606498718261719 Accuracy: 12.5\n",
      "[val] Epoch: 11/80 Loss: 4.186024788560383 Accuracy: 17.093008002098912 Execution time: 193.02628993988037\n",
      "[train] Epoch: 12/80 Batch: 100/952 Loss: 3.167717933654785 Accuracy: 28.125\n",
      "[train] Epoch: 12/80 Batch: 200/952 Loss: 2.9102871417999268 Accuracy: 21.875\n",
      "[train] Epoch: 12/80 Batch: 300/952 Loss: 2.9515798091888428 Accuracy: 28.125\n",
      "[train] Epoch: 12/80 Batch: 400/952 Loss: 3.0546963214874268 Accuracy: 34.375\n",
      "[train] Epoch: 12/80 Batch: 500/952 Loss: 3.6841812133789062 Accuracy: 21.875\n",
      "[train] Epoch: 12/80 Batch: 600/952 Loss: 2.478210210800171 Accuracy: 43.75\n",
      "[train] Epoch: 12/80 Batch: 700/952 Loss: 2.7257909774780273 Accuracy: 37.5\n",
      "[train] Epoch: 12/80 Batch: 800/952 Loss: 3.3826794624328613 Accuracy: 25.0\n",
      "[train] Epoch: 12/80 Batch: 900/952 Loss: 3.5761146545410156 Accuracy: 15.625\n",
      "[train] Epoch: 12/80 Loss: 3.1747290654267655 Accuracy: 28.742416789637645 Execution time: 733.252542257309\n",
      "[val] Epoch: 12/80 Batch: 100/238 Loss: 3.4156296253204346 Accuracy: 21.875\n",
      "[val] Epoch: 12/80 Batch: 200/238 Loss: 4.500734806060791 Accuracy: 6.25\n",
      "[val] Epoch: 12/80 Loss: 4.224929119992347 Accuracy: 17.06677161222616 Execution time: 186.78276658058167\n",
      "[train] Epoch: 13/80 Batch: 100/952 Loss: 2.9657723903656006 Accuracy: 34.375\n",
      "[train] Epoch: 13/80 Batch: 200/952 Loss: 2.863375663757324 Accuracy: 25.0\n",
      "[train] Epoch: 13/80 Batch: 300/952 Loss: 3.331225872039795 Accuracy: 25.0\n",
      "[train] Epoch: 13/80 Batch: 400/952 Loss: 3.036482334136963 Accuracy: 37.5\n",
      "[train] Epoch: 13/80 Batch: 500/952 Loss: 2.8603134155273438 Accuracy: 34.375\n",
      "[train] Epoch: 13/80 Batch: 600/952 Loss: 3.0036449432373047 Accuracy: 18.75\n",
      "[train] Epoch: 13/80 Batch: 700/952 Loss: 3.097407579421997 Accuracy: 25.0\n",
      "[train] Epoch: 13/80 Batch: 800/952 Loss: 3.6731858253479004 Accuracy: 28.125\n",
      "[train] Epoch: 13/80 Batch: 900/952 Loss: 3.7996227741241455 Accuracy: 25.0\n",
      "[train] Epoch: 13/80 Loss: 3.038000077477321 Accuracy: 31.228070175438596 Execution time: 752.0085802078247\n",
      "[val] Epoch: 13/80 Batch: 100/238 Loss: 4.317099571228027 Accuracy: 12.5\n",
      "[val] Epoch: 13/80 Batch: 200/238 Loss: 4.222968578338623 Accuracy: 25.0\n",
      "[val] Epoch: 13/80 Loss: 4.274931342054755 Accuracy: 17.197953561589927 Execution time: 191.34905910491943\n",
      "[train] Epoch: 14/80 Batch: 100/952 Loss: 3.0949831008911133 Accuracy: 25.0\n",
      "[train] Epoch: 14/80 Batch: 200/952 Loss: 2.681410551071167 Accuracy: 37.5\n",
      "[train] Epoch: 14/80 Batch: 300/952 Loss: 2.9464340209960938 Accuracy: 31.25\n",
      "[train] Epoch: 14/80 Batch: 400/952 Loss: 2.9980716705322266 Accuracy: 31.25\n",
      "[train] Epoch: 14/80 Batch: 500/952 Loss: 2.351581573486328 Accuracy: 43.75\n",
      "[train] Epoch: 14/80 Batch: 600/952 Loss: 2.9924302101135254 Accuracy: 21.875\n",
      "[train] Epoch: 14/80 Batch: 700/952 Loss: 3.1909360885620117 Accuracy: 34.375\n",
      "[train] Epoch: 14/80 Batch: 800/952 Loss: 3.134104013442993 Accuracy: 31.25\n",
      "[train] Epoch: 14/80 Batch: 900/952 Loss: 3.607171058654785 Accuracy: 12.5\n",
      "[train] Epoch: 14/80 Loss: 2.9075773027416605 Accuracy: 33.10378750614855 Execution time: 751.7837212085724\n",
      "[val] Epoch: 14/80 Batch: 100/238 Loss: 4.181173801422119 Accuracy: 18.75\n",
      "[val] Epoch: 14/80 Batch: 200/238 Loss: 4.079685211181641 Accuracy: 15.625\n",
      "[val] Epoch: 14/80 Loss: 4.31229036629677 Accuracy: 16.68634395907123 Execution time: 191.37609481811523\n",
      "[train] Epoch: 15/80 Batch: 100/952 Loss: 2.3672609329223633 Accuracy: 43.75\n",
      "[train] Epoch: 15/80 Batch: 200/952 Loss: 3.2785463333129883 Accuracy: 34.375\n",
      "[train] Epoch: 15/80 Batch: 300/952 Loss: 3.1666769981384277 Accuracy: 25.0\n",
      "[train] Epoch: 15/80 Batch: 400/952 Loss: 2.453603744506836 Accuracy: 43.75\n",
      "[train] Epoch: 15/80 Batch: 500/952 Loss: 2.5384342670440674 Accuracy: 34.375\n",
      "[train] Epoch: 15/80 Batch: 600/952 Loss: 3.1669490337371826 Accuracy: 28.125\n",
      "[train] Epoch: 15/80 Batch: 700/952 Loss: 2.5933899879455566 Accuracy: 34.375\n",
      "[train] Epoch: 15/80 Batch: 800/952 Loss: 2.9518938064575195 Accuracy: 31.25\n",
      "[train] Epoch: 15/80 Batch: 900/952 Loss: 2.0182089805603027 Accuracy: 56.25\n",
      "[train] Epoch: 15/80 Loss: 2.777490450660016 Accuracy: 35.53369404820462 Execution time: 750.2971522808075\n",
      "[val] Epoch: 15/80 Batch: 100/238 Loss: 4.691145896911621 Accuracy: 18.75\n",
      "[val] Epoch: 15/80 Batch: 200/238 Loss: 4.159120082855225 Accuracy: 25.0\n",
      "[val] Epoch: 15/80 Loss: 4.394076510863121 Accuracy: 16.76505312868949 Execution time: 192.25660514831543\n",
      "[train] Epoch: 16/80 Batch: 100/952 Loss: 3.0239195823669434 Accuracy: 31.25\n",
      "[train] Epoch: 16/80 Batch: 200/952 Loss: 2.857605218887329 Accuracy: 28.125\n",
      "[train] Epoch: 16/80 Batch: 300/952 Loss: 3.054837703704834 Accuracy: 31.25\n",
      "[train] Epoch: 16/80 Batch: 400/952 Loss: 2.64774227142334 Accuracy: 37.5\n",
      "[train] Epoch: 16/80 Batch: 500/952 Loss: 3.4951155185699463 Accuracy: 18.75\n",
      "[train] Epoch: 16/80 Batch: 600/952 Loss: 2.351937770843506 Accuracy: 40.625\n",
      "[train] Epoch: 16/80 Batch: 700/952 Loss: 2.3797810077667236 Accuracy: 46.875\n",
      "[train] Epoch: 16/80 Batch: 800/952 Loss: 3.0195236206054688 Accuracy: 31.25\n",
      "[train] Epoch: 16/80 Batch: 900/952 Loss: 2.963573694229126 Accuracy: 37.5\n",
      "[train] Epoch: 16/80 Loss: 2.644636183383915 Accuracy: 37.67830791933104 Execution time: 748.6974861621857\n",
      "[val] Epoch: 16/80 Batch: 100/238 Loss: 3.8449642658233643 Accuracy: 31.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] Epoch: 16/80 Batch: 200/238 Loss: 5.720288276672363 Accuracy: 12.5\n",
      "[val] Epoch: 16/80 Loss: 4.522883385572317 Accuracy: 16.096025186934277 Execution time: 192.76015639305115\n",
      "[train] Epoch: 17/80 Batch: 100/952 Loss: 2.54832124710083 Accuracy: 31.25\n",
      "[train] Epoch: 17/80 Batch: 200/952 Loss: 2.7777369022369385 Accuracy: 37.5\n",
      "[train] Epoch: 17/80 Batch: 300/952 Loss: 2.6741151809692383 Accuracy: 43.75\n",
      "[train] Epoch: 17/80 Batch: 400/952 Loss: 2.316897392272949 Accuracy: 53.125\n",
      "[train] Epoch: 17/80 Batch: 500/952 Loss: 2.1797444820404053 Accuracy: 50.0\n",
      "[train] Epoch: 17/80 Batch: 600/952 Loss: 2.9349756240844727 Accuracy: 31.25\n",
      "[train] Epoch: 17/80 Batch: 700/952 Loss: 3.059478759765625 Accuracy: 28.125\n",
      "[train] Epoch: 17/80 Batch: 800/952 Loss: 2.764176607131958 Accuracy: 34.375\n",
      "[train] Epoch: 17/80 Batch: 900/952 Loss: 2.2660984992980957 Accuracy: 46.875\n",
      "[train] Epoch: 17/80 Loss: 2.5079845950728186 Accuracy: 40.370552549598294 Execution time: 757.4389278888702\n",
      "[val] Epoch: 17/80 Batch: 100/238 Loss: 4.087262153625488 Accuracy: 18.75\n",
      "[val] Epoch: 17/80 Batch: 200/238 Loss: 4.382408618927002 Accuracy: 28.125\n",
      "[val] Epoch: 17/80 Loss: 4.597716766721727 Accuracy: 16.240325331234423 Execution time: 194.8809154033661\n",
      "[train] Epoch: 18/80 Batch: 100/952 Loss: 2.5068089962005615 Accuracy: 43.75\n",
      "[train] Epoch: 18/80 Batch: 200/952 Loss: 2.4706902503967285 Accuracy: 34.375\n",
      "[train] Epoch: 18/80 Batch: 300/952 Loss: 2.6875593662261963 Accuracy: 40.625\n",
      "[train] Epoch: 18/80 Batch: 400/952 Loss: 2.461237907409668 Accuracy: 37.5\n",
      "[train] Epoch: 18/80 Batch: 500/952 Loss: 1.857052206993103 Accuracy: 53.125\n",
      "[train] Epoch: 18/80 Batch: 600/952 Loss: 2.175786256790161 Accuracy: 46.875\n",
      "[train] Epoch: 18/80 Batch: 700/952 Loss: 2.1684181690216064 Accuracy: 50.0\n",
      "[train] Epoch: 18/80 Batch: 800/952 Loss: 2.3709640502929688 Accuracy: 43.75\n",
      "[train] Epoch: 18/80 Batch: 900/952 Loss: 2.1554453372955322 Accuracy: 40.625\n",
      "[train] Epoch: 18/80 Loss: 2.37041013761121 Accuracy: 43.131660928020985 Execution time: 764.2267863750458\n",
      "[val] Epoch: 18/80 Batch: 100/238 Loss: 4.280574321746826 Accuracy: 18.75\n",
      "[val] Epoch: 18/80 Batch: 200/238 Loss: 4.520103454589844 Accuracy: 21.875\n",
      "[val] Epoch: 18/80 Loss: 4.70034675042679 Accuracy: 15.466351829988193 Execution time: 193.1979205608368\n",
      "[train] Epoch: 19/80 Batch: 100/952 Loss: 2.1168248653411865 Accuracy: 40.625\n",
      "[train] Epoch: 19/80 Batch: 200/952 Loss: 2.1190409660339355 Accuracy: 65.625\n",
      "[train] Epoch: 19/80 Batch: 300/952 Loss: 2.611370801925659 Accuracy: 40.625\n",
      "[train] Epoch: 19/80 Batch: 400/952 Loss: 2.492237091064453 Accuracy: 40.625\n",
      "[train] Epoch: 19/80 Batch: 500/952 Loss: 2.2864699363708496 Accuracy: 40.625\n",
      "[train] Epoch: 19/80 Batch: 600/952 Loss: 1.757975459098816 Accuracy: 56.25\n",
      "[train] Epoch: 19/80 Batch: 700/952 Loss: 2.228317975997925 Accuracy: 50.0\n",
      "[train] Epoch: 19/80 Batch: 800/952 Loss: 2.278069257736206 Accuracy: 46.875\n",
      "[train] Epoch: 19/80 Batch: 900/952 Loss: 2.260129928588867 Accuracy: 46.875\n",
      "[train] Epoch: 19/80 Loss: 2.2347967730445695 Accuracy: 45.850139367109364 Execution time: 762.0748136043549\n",
      "[val] Epoch: 19/80 Batch: 100/238 Loss: 4.7740583419799805 Accuracy: 9.375\n",
      "[val] Epoch: 19/80 Batch: 200/238 Loss: 5.28823709487915 Accuracy: 12.5\n",
      "[val] Epoch: 19/80 Loss: 4.829363679191532 Accuracy: 15.466351829988193 Execution time: 195.01875567436218\n",
      "[train] Epoch: 20/80 Batch: 100/952 Loss: 2.301915168762207 Accuracy: 37.5\n",
      "[train] Epoch: 20/80 Batch: 200/952 Loss: 1.8193978071212769 Accuracy: 62.5\n",
      "[train] Epoch: 20/80 Batch: 300/952 Loss: 2.0724523067474365 Accuracy: 37.5\n",
      "[train] Epoch: 20/80 Batch: 400/952 Loss: 2.3515658378601074 Accuracy: 25.0\n",
      "[train] Epoch: 20/80 Batch: 500/952 Loss: 2.116750955581665 Accuracy: 37.5\n",
      "[train] Epoch: 20/80 Batch: 600/952 Loss: 2.6578421592712402 Accuracy: 37.5\n",
      "[train] Epoch: 20/80 Batch: 700/952 Loss: 2.2363932132720947 Accuracy: 50.0\n",
      "[train] Epoch: 20/80 Batch: 800/952 Loss: 2.189164400100708 Accuracy: 50.0\n",
      "[train] Epoch: 20/80 Batch: 900/952 Loss: 2.3900134563446045 Accuracy: 46.875\n",
      "[train] Epoch: 20/80 Loss: 2.111523000676821 Accuracy: 48.08657156910969 Execution time: 760.2610170841217\n",
      "[val] Epoch: 20/80 Batch: 100/238 Loss: 4.727505683898926 Accuracy: 12.5\n",
      "[val] Epoch: 20/80 Batch: 200/238 Loss: 4.369535446166992 Accuracy: 18.75\n",
      "[val] Epoch: 20/80 Loss: 4.9962567976574155 Accuracy: 15.623770169224715 Execution time: 193.91126799583435\n",
      "[train] Epoch: 21/80 Batch: 100/952 Loss: 1.968625545501709 Accuracy: 50.0\n",
      "[train] Epoch: 21/80 Batch: 200/952 Loss: 2.2092483043670654 Accuracy: 46.875\n",
      "[train] Epoch: 21/80 Batch: 300/952 Loss: 2.288126230239868 Accuracy: 37.5\n",
      "[train] Epoch: 21/80 Batch: 400/952 Loss: 1.8141486644744873 Accuracy: 53.125\n",
      "[train] Epoch: 21/80 Batch: 500/952 Loss: 2.309678316116333 Accuracy: 53.125\n",
      "[train] Epoch: 21/80 Batch: 600/952 Loss: 1.788576602935791 Accuracy: 59.375\n",
      "[train] Epoch: 21/80 Batch: 700/952 Loss: 2.074488878250122 Accuracy: 50.0\n",
      "[train] Epoch: 21/80 Batch: 800/952 Loss: 1.9503058195114136 Accuracy: 46.875\n",
      "[train] Epoch: 21/80 Batch: 900/952 Loss: 1.1132805347442627 Accuracy: 78.125\n",
      "[train] Epoch: 21/80 Loss: 1.972062128945479 Accuracy: 51.29693392359403 Execution time: 764.2357153892517\n",
      "[val] Epoch: 21/80 Batch: 100/238 Loss: 4.73052978515625 Accuracy: 15.625\n",
      "[val] Epoch: 21/80 Batch: 200/238 Loss: 5.050045967102051 Accuracy: 12.5\n",
      "[val] Epoch: 21/80 Loss: 5.071287704361684 Accuracy: 15.059687786960515 Execution time: 194.68500399589539\n",
      "[train] Epoch: 22/80 Batch: 100/952 Loss: 1.931849718093872 Accuracy: 50.0\n",
      "[train] Epoch: 22/80 Batch: 200/952 Loss: 1.8806676864624023 Accuracy: 59.375\n",
      "[train] Epoch: 22/80 Batch: 300/952 Loss: 1.7326371669769287 Accuracy: 62.5\n",
      "[train] Epoch: 22/80 Batch: 400/952 Loss: 2.2835466861724854 Accuracy: 46.875\n",
      "[train] Epoch: 22/80 Batch: 500/952 Loss: 2.1566481590270996 Accuracy: 46.875\n",
      "[train] Epoch: 22/80 Batch: 600/952 Loss: 1.8132612705230713 Accuracy: 50.0\n",
      "[train] Epoch: 22/80 Batch: 700/952 Loss: 1.7746756076812744 Accuracy: 50.0\n",
      "[train] Epoch: 22/80 Batch: 800/952 Loss: 1.6958837509155273 Accuracy: 62.5\n",
      "[train] Epoch: 22/80 Batch: 900/952 Loss: 1.8310041427612305 Accuracy: 59.375\n",
      "[train] Epoch: 22/80 Loss: 1.8518889259404678 Accuracy: 53.72028201344483 Execution time: 764.5904579162598\n",
      "[val] Epoch: 22/80 Batch: 100/238 Loss: 5.031399250030518 Accuracy: 12.5\n",
      "[val] Epoch: 22/80 Batch: 200/238 Loss: 5.008352279663086 Accuracy: 18.75\n",
      "[val] Epoch: 22/80 Loss: 5.243217783525956 Accuracy: 14.416896235078053 Execution time: 194.7490782737732\n",
      "[train] Epoch: 23/80 Batch: 100/952 Loss: 1.5557904243469238 Accuracy: 65.625\n",
      "[train] Epoch: 23/80 Batch: 200/952 Loss: 2.167496681213379 Accuracy: 50.0\n",
      "[train] Epoch: 23/80 Batch: 300/952 Loss: 2.333441972732544 Accuracy: 43.75\n",
      "[train] Epoch: 23/80 Batch: 400/952 Loss: 1.5124174356460571 Accuracy: 62.5\n",
      "[train] Epoch: 23/80 Batch: 500/952 Loss: 1.9060916900634766 Accuracy: 56.25\n",
      "[train] Epoch: 23/80 Batch: 600/952 Loss: 1.3665387630462646 Accuracy: 71.875\n",
      "[train] Epoch: 23/80 Batch: 700/952 Loss: 1.758909821510315 Accuracy: 43.75\n",
      "[train] Epoch: 23/80 Batch: 800/952 Loss: 1.4635469913482666 Accuracy: 62.5\n",
      "[train] Epoch: 23/80 Batch: 900/952 Loss: 1.6376090049743652 Accuracy: 59.375\n",
      "[train] Epoch: 23/80 Loss: 1.7180953780595036 Accuracy: 56.7765207411051 Execution time: 766.9167585372925\n",
      "[val] Epoch: 23/80 Batch: 100/238 Loss: 4.994673252105713 Accuracy: 18.75\n",
      "[val] Epoch: 23/80 Batch: 200/238 Loss: 4.568479537963867 Accuracy: 28.125\n",
      "[val] Epoch: 23/80 Loss: 5.453933413566981 Accuracy: 15.112160566706022 Execution time: 193.5804090499878\n",
      "[train] Epoch: 24/80 Batch: 100/952 Loss: 1.6742937564849854 Accuracy: 68.75\n",
      "[train] Epoch: 24/80 Batch: 200/952 Loss: 1.3560048341751099 Accuracy: 59.375\n",
      "[train] Epoch: 24/80 Batch: 300/952 Loss: 1.8959423303604126 Accuracy: 56.25\n",
      "[train] Epoch: 24/80 Batch: 400/952 Loss: 1.2510395050048828 Accuracy: 62.5\n",
      "[train] Epoch: 24/80 Batch: 500/952 Loss: 1.7607245445251465 Accuracy: 62.5\n",
      "[train] Epoch: 24/80 Batch: 600/952 Loss: 1.9362205266952515 Accuracy: 50.0\n",
      "[train] Epoch: 24/80 Batch: 700/952 Loss: 1.5715168714523315 Accuracy: 65.625\n",
      "[train] Epoch: 24/80 Batch: 800/952 Loss: 1.3477110862731934 Accuracy: 56.25\n",
      "[train] Epoch: 24/80 Batch: 900/952 Loss: 1.4065121412277222 Accuracy: 62.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Epoch: 24/80 Loss: 1.60745151467862 Accuracy: 59.350713231677325 Execution time: 751.4768993854523\n",
      "[val] Epoch: 24/80 Batch: 100/238 Loss: 6.2420854568481445 Accuracy: 15.625\n",
      "[val] Epoch: 24/80 Batch: 200/238 Loss: 4.485657691955566 Accuracy: 15.625\n",
      "[val] Epoch: 24/80 Loss: 5.625930802146234 Accuracy: 14.088941361668635 Execution time: 192.47737288475037\n",
      "[train] Epoch: 25/80 Batch: 100/952 Loss: 1.4848395586013794 Accuracy: 53.125\n",
      "[train] Epoch: 25/80 Batch: 200/952 Loss: 0.7980847358703613 Accuracy: 87.5\n",
      "[train] Epoch: 25/80 Batch: 300/952 Loss: 1.469244122505188 Accuracy: 65.625\n",
      "[train] Epoch: 25/80 Batch: 400/952 Loss: 1.5138001441955566 Accuracy: 71.875\n",
      "[train] Epoch: 25/80 Batch: 500/952 Loss: 1.4664453268051147 Accuracy: 62.5\n",
      "[train] Epoch: 25/80 Batch: 600/952 Loss: 1.245239019393921 Accuracy: 68.75\n",
      "[train] Epoch: 25/80 Batch: 700/952 Loss: 1.3772474527359009 Accuracy: 68.75\n",
      "[train] Epoch: 25/80 Batch: 800/952 Loss: 1.3151147365570068 Accuracy: 56.25\n",
      "[train] Epoch: 25/80 Batch: 900/952 Loss: 1.0460598468780518 Accuracy: 71.875\n",
      "[train] Epoch: 25/80 Loss: 1.491897119915198 Accuracy: 61.7806197737334 Execution time: 766.4207475185394\n",
      "[val] Epoch: 25/80 Batch: 100/238 Loss: 6.033069610595703 Accuracy: 9.375\n",
      "[val] Epoch: 25/80 Batch: 200/238 Loss: 6.352967262268066 Accuracy: 9.375\n",
      "[val] Epoch: 25/80 Loss: 5.84818308256155 Accuracy: 13.131313131313131 Execution time: 199.1431748867035\n",
      "[train] Epoch: 26/80 Batch: 100/952 Loss: 1.3540412187576294 Accuracy: 59.375\n",
      "[train] Epoch: 26/80 Batch: 200/952 Loss: 0.8496293425559998 Accuracy: 81.25\n",
      "[train] Epoch: 26/80 Batch: 300/952 Loss: 1.4116501808166504 Accuracy: 68.75\n",
      "[train] Epoch: 26/80 Batch: 400/952 Loss: 1.5014930963516235 Accuracy: 71.875\n",
      "[train] Epoch: 26/80 Batch: 500/952 Loss: 1.6209415197372437 Accuracy: 62.5\n",
      "[train] Epoch: 26/80 Batch: 600/952 Loss: 1.0274853706359863 Accuracy: 68.75\n",
      "[train] Epoch: 26/80 Batch: 700/952 Loss: 1.5493959188461304 Accuracy: 65.625\n",
      "[train] Epoch: 26/80 Batch: 800/952 Loss: 1.050782322883606 Accuracy: 75.0\n",
      "[train] Epoch: 26/80 Batch: 900/952 Loss: 1.9035489559173584 Accuracy: 53.125\n",
      "[train] Epoch: 26/80 Loss: 1.3817280939004914 Accuracy: 64.81718314477783 Execution time: 783.6528844833374\n",
      "[val] Epoch: 26/80 Batch: 100/238 Loss: 5.557567596435547 Accuracy: 12.5\n",
      "[val] Epoch: 26/80 Batch: 200/238 Loss: 6.907705307006836 Accuracy: 6.25\n",
      "[val] Epoch: 26/80 Loss: 6.101327627142724 Accuracy: 13.852813852813853 Execution time: 202.03336024284363\n",
      "[train] Epoch: 27/80 Batch: 100/952 Loss: 1.2056905031204224 Accuracy: 71.875\n",
      "[train] Epoch: 27/80 Batch: 200/952 Loss: 1.441235065460205 Accuracy: 75.0\n",
      "[train] Epoch: 27/80 Batch: 300/952 Loss: 1.3082900047302246 Accuracy: 59.375\n",
      "[train] Epoch: 27/80 Batch: 400/952 Loss: 1.1657028198242188 Accuracy: 81.25\n",
      "[train] Epoch: 27/80 Batch: 500/952 Loss: 1.2608025074005127 Accuracy: 62.5\n",
      "[train] Epoch: 27/80 Batch: 600/952 Loss: 1.0320522785186768 Accuracy: 71.875\n",
      "[train] Epoch: 27/80 Batch: 700/952 Loss: 0.9907264709472656 Accuracy: 75.0\n",
      "[train] Epoch: 27/80 Batch: 800/952 Loss: 1.213189721107483 Accuracy: 68.75\n",
      "[train] Epoch: 27/80 Batch: 900/952 Loss: 1.1327182054519653 Accuracy: 65.625\n",
      "[train] Epoch: 27/80 Loss: 1.271269008100453 Accuracy: 67.13559599934416 Execution time: 805.7684638500214\n",
      "[val] Epoch: 27/80 Batch: 100/238 Loss: 5.88056755065918 Accuracy: 6.25\n",
      "[val] Epoch: 27/80 Batch: 200/238 Loss: 5.848446846008301 Accuracy: 6.25\n",
      "[val] Epoch: 27/80 Loss: 6.282162192548712 Accuracy: 13.34120425029516 Execution time: 206.7948522567749\n",
      "[train] Epoch: 28/80 Batch: 100/952 Loss: 0.8928306102752686 Accuracy: 81.25\n",
      "[train] Epoch: 28/80 Batch: 200/952 Loss: 1.2900094985961914 Accuracy: 71.875\n",
      "[train] Epoch: 28/80 Batch: 300/952 Loss: 0.6870704293251038 Accuracy: 87.5\n",
      "[train] Epoch: 28/80 Batch: 400/952 Loss: 0.8641318082809448 Accuracy: 81.25\n",
      "[train] Epoch: 28/80 Batch: 500/952 Loss: 1.1711047887802124 Accuracy: 65.625\n",
      "[train] Epoch: 28/80 Batch: 600/952 Loss: 0.7471381425857544 Accuracy: 81.25\n",
      "[train] Epoch: 28/80 Batch: 700/952 Loss: 1.025923490524292 Accuracy: 68.75\n",
      "[train] Epoch: 28/80 Batch: 800/952 Loss: 1.5851469039916992 Accuracy: 68.75\n",
      "[train] Epoch: 28/80 Batch: 900/952 Loss: 1.002386450767517 Accuracy: 81.25\n",
      "[train] Epoch: 28/80 Loss: 1.1663159300526902 Accuracy: 70.06722413510411 Execution time: 807.6466147899628\n",
      "[val] Epoch: 28/80 Batch: 100/238 Loss: 5.025350570678711 Accuracy: 18.75\n",
      "[val] Epoch: 28/80 Batch: 200/238 Loss: 7.459234714508057 Accuracy: 0.0\n",
      "[val] Epoch: 28/80 Loss: 6.43219657296445 Accuracy: 13.2624950806769 Execution time: 201.98455691337585\n",
      "[train] Epoch: 29/80 Batch: 100/952 Loss: 0.7262575626373291 Accuracy: 84.375\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "with open('datasets/processed/hico/anno_list.json') as f:\n",
    "    anno_list = json.load(f)\n",
    "    \n",
    "with open('datasets/processed/hico/hoi_list.json') as f:\n",
    "    hoi_list = json.load(f)\n",
    "    \n",
    "img_cache = {} # format {key: [human, object, pairwise]}\n",
    "img_cache_counter = 0\n",
    "    \n",
    "print('Training has started!')\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for phase in ['train', 'val']:\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        idx = 0\n",
    "        \n",
    "        for data in dataloader[phase]:\n",
    "            train_data = data\n",
    "            img_name = train_data['img_name']\n",
    "            \n",
    "            labels = np.zeros((batch_size, 600))\n",
    "            batch_correct = 0\n",
    "            for i in range(batch_size):\n",
    "                # Get image data\n",
    "                parsed_img_name = img_name[i].split(\".\")[0]\n",
    "                img = [x for x in anno_list if x['global_id'] == parsed_img_name][0]\n",
    "                img = img['hois'][0]\n",
    "                img_id = int(img['id']) - 1\n",
    "                labels[i][img_id] = 1\n",
    "                human_bboxes = img['human_bboxes']\n",
    "                object_bboxes = img['object_bboxes']\n",
    "\n",
    "                # Apply masks to images [with caching]\n",
    "                src_img_path = TRAIN_IMG_PATH + parsed_img_name + '.jpg'\n",
    "                if src_img_path in img_cache: # Use cache if available\n",
    "                    human_bbox_img, obj_bbox_img, pairwise_bbox_img = img_cache[src_img_path]\n",
    "                else:\n",
    "                    src = cv2.imread(src_img_path)\n",
    "                    human_mask = np.zeros_like(src)\n",
    "                    for bbox in human_bboxes:\n",
    "                        cv2.rectangle(human_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                    human_bbox_img = cv2.bitwise_and(src, human_mask, mask=None)\n",
    "\n",
    "                    obj_mask = np.zeros_like(src)\n",
    "                    pairwise_mask = human_mask\n",
    "                    for bbox in object_bboxes:\n",
    "                        cv2.rectangle(obj_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                        cv2.rectangle(pairwise_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                    obj_bbox_img = cv2.bitwise_and(src, obj_mask, mask=None)\n",
    "                    pairwise_bbox_img = cv2.bitwise_and(src, pairwise_mask, mask=None)\n",
    "                    \n",
    "                    # Add to cache if within limits\n",
    "                    if not max_img_cache_size or img_cache_counter < max_img_cache_size:\n",
    "                        img_cache[src_img_path] = [human_bbox_img, obj_bbox_img, pairwise_bbox_img]\n",
    "                        img_cache_counter += 1\n",
    "\n",
    "                '''\n",
    "                # Visualization of masks\n",
    "                f, axarr = plt.subplots(1,3)\n",
    "\n",
    "                human_bbox_rgb = cv2.cvtColor(human_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[0].imshow(human_bbox_rgb)\n",
    "\n",
    "                object_mask_rgb = cv2.cvtColor(obj_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[1].imshow(object_mask_rgb)\n",
    "\n",
    "                pairwise_rgb = cv2.cvtColor(pairwise_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[2].imshow(pairwise_rgb)\n",
    "\n",
    "                plt.show()\n",
    "                f.clf()\n",
    "                '''\n",
    "\n",
    "                human_bbox_img = cv2.resize(human_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "                obj_bbox_img = cv2.resize(obj_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "                pairwise_bbox_img = cv2.resize(pairwise_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                human_bbox_img = torch.from_numpy(human_bbox_img).to(device)\n",
    "                obj_bbox_img = torch.from_numpy(obj_bbox_img).to(device)\n",
    "                pairwise_bbox_img = torch.from_numpy(pairwise_bbox_img).to(device)\n",
    "\n",
    "                if i == 0:\n",
    "                    res_human_input = human_bbox_img.unsqueeze(0)\n",
    "                    res_obj_input = obj_bbox_img.unsqueeze(0)\n",
    "                    res_pairwise_input = pairwise_bbox_img.unsqueeze(0)\n",
    "                else:\n",
    "                    res_human_input = torch.cat((res_human_input, human_bbox_img.unsqueeze(0)), dim=0)\n",
    "                    res_obj_input = torch.cat((res_obj_input, obj_bbox_img.unsqueeze(0)), dim=0)\n",
    "                    res_pairwise_input = torch.cat((res_pairwise_input, pairwise_bbox_img.unsqueeze(0)), dim=0)\n",
    "\n",
    "            res_human_input = res_human_input.permute([0,3,1,2]).float().to(device)\n",
    "            res_obj_input = res_obj_input.permute([0,3,1,2]).float().to(device)\n",
    "            res_pairwise_input = res_pairwise_input.permute([0,3,1,2]).float().to(device)\n",
    "            labels = torch.from_numpy(labels).long().to(device)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                # Initial train loop\n",
    "                model.train()\n",
    "                model.zero_grad()\n",
    "                \n",
    "                # Forward pass: human, objects, pairwise streams\n",
    "                outputs = model.forward(res_human_input, res_obj_input, res_pairwise_input)\n",
    "                loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                ground_labels = torch.max(labels, 1)[1]\n",
    "                for accuracy_iterator in range(len(ground_labels)):\n",
    "                    if preds[accuracy_iterator] == ground_labels[accuracy_iterator]:\n",
    "                        batch_correct += 1\n",
    "                \n",
    "            else:\n",
    "                # Evaluation after train loop\n",
    "                model.eval()\n",
    "                with torch.no_grad(): # Disable gradients for validation\n",
    "                    outputs = model.forward(res_human_input, res_obj_input, res_pairwise_input)\n",
    "                    loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "                    \n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    ground_labels = torch.max(labels, 1)[1]\n",
    "                    for accuracy_iterator in range(len(ground_labels)):\n",
    "                        if preds[accuracy_iterator] == ground_labels[accuracy_iterator]:\n",
    "                            batch_correct += 1\n",
    "                    \n",
    "            # Accumulate loss of each batch (average * batch size)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_correct += batch_correct\n",
    "            \n",
    "            # Print out status per print_batch_every\n",
    "            idx += 1\n",
    "            if (idx % print_batch_every) == 0:\n",
    "                print(\"[{}] Epoch: {}/{} Batch: {}/{} Loss: {} Accuracy: {}\".format(\\\n",
    "                        phase, epoch+1, epochs, idx, len(dataloader[phase]), \\\n",
    "                        loss.item(), 100 * batch_correct / batch_size))\n",
    "            \n",
    "        # Epoch loss and accuracy\n",
    "        epoch_loss = running_loss / len(dataset[phase])\n",
    "        epoch_accuracy = 100 * running_correct / len(dataset[phase])\n",
    "        \n",
    "        # Log trainval data for visualization\n",
    "        if phase == 'train':\n",
    "            train_loss = epoch_loss \n",
    "            train_accuracy = epoch_accuracy\n",
    "        else:\n",
    "            writer.add_scalars('trainval_loss_epoch', {'train': train_loss, 'val': epoch_loss}, epoch)\n",
    "            writer.add_scalars('trainval_accuracy_epoch', {'train': train_accuracy, 'val': epoch_accuracy}, epoch)\n",
    "            \n",
    "        # Output data per print_epoch_every\n",
    "        if (epoch % print_epoch_every) == 0:\n",
    "            end_time = time.time()\n",
    "            print(\"[{}] Epoch: {}/{} Loss: {} Accuracy: {} Execution time: {}\".format(\\\n",
    "                    phase, epoch+1, epochs, epoch_loss, epoch_accuracy, (end_time-start_time)))\n",
    "    \n",
    "    # Save the model per save_every\n",
    "    if epoch_loss<0.0405 or epoch % save_every == (save_every - 1) and epoch >= (10-1):\n",
    "        checkpoint = { \n",
    "                        'lr': initial_lr,\n",
    "                       'b_s': batch_size,\n",
    "                 'feat_type': feat_type,\n",
    "                'state_dict': model.state_dict()\n",
    "        }\n",
    "        save_name = \"checkpoint_\" + str(epoch+1) + '_epoch.pth'\n",
    "        torch.save(checkpoint, os.path.join(save_dir, exp_ver, 'epoch_train', save_name))\n",
    "        \n",
    "print('Finishing training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close visualization\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interact",
   "language": "python",
   "name": "interact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
