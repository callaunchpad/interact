{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import cv2\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "import spatial\n",
    "from model.bgnet_model import AGRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, rpn_post_nms_top_n_test=200, \\\n",
    "                                                                 box_batch_size_per_image=128, box_score_thresh=0.1, box_nms_thresh=0.3)\n",
    "faster_rcnn.cuda()\n",
    "faster_rcnn.eval()\n",
    "\n",
    "node_num = []\n",
    "features = None\n",
    "spatial_feat = None\n",
    "word2vec_emb = None\n",
    "roi_labels = None\n",
    "bg = None\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints/run_bg_final_final/v8/epoch_train/checkpoint_21_epoch.pth\", map_location=device)\n",
    "model = AGRNN(feat_type=checkpoint['feat_type'], bias=checkpoint['bias'], bn=checkpoint['bn'], dropout=checkpoint['dropout'], multi_attn=checkpoint['multi_head'], layer=checkpoint['layers'], diff_edge=checkpoint['diff_edge']) #2 )\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "img = cv2.imread(\"HICO_train2015_00000019.jpg\")\n",
    "# img = cv2.imread(\"HICO_test2015_00000016.jpg\")\n",
    "word2vec = h5py.File(\"datasets/processed/hico/hico_word2vec.hdf5\", 'r')\n",
    "# coco_dict = ['__background__',\n",
    "#     'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "#     'traffic_light', 'fire_hydrant', 'stop_sign', 'parking_meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "#     'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "#     'suitcase', 'frisbee', 'skis', 'snowboard', 'sports_ball', 'kite', 'baseball_bat', 'baseball_glove',\n",
    "#     'skateboard', 'surfboard', 'tennis_racket', 'bottle', 'wine_glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "#     'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot_dog', 'pizza', 'donut',\n",
    "#     'cake', 'chair', 'couch', 'potted_plant', 'bed', 'dining_table', 'toilet', 'tv', 'laptop', 'mouse',\n",
    "#     'remote', 'keyboard', 'cell_phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
    "#     'clock', 'vase', 'scissors', 'teddy_bear', 'hair_drier', 'toothbrush']\n",
    "\n",
    "coco_dict = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic_light', 'fire_hydrant', 'N/A', 'stop_sign',\n",
    "    'parking_meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball_bat', 'baseball_glove', 'skateboard', 'surfboard', 'tennis_racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot_dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted_plant', 'bed', 'N/A', 'dining_table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell_phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy_bear', 'hair_drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "verbs = [\"adjust\", \"assemble\", \"block\", \"blow\", \"board\", \"break\", \"brush_with\", \"buy\", \"carry\", \"catch\", \"chase\", \n",
    "         \"check\", \"clean\", \"control\", \"cook\", \"cut\", \"cut_with\", \"direct\", \"drag\", \"dribble\", \"drink_with\", \"drive\",\n",
    "         \"dry\", \"eat\", \"eat_at\", \"exit\", \"feed\", \"fill\", \"flip\", \"flush\", \"fly\", \"greet\", \"grind\", \"groom\", \"herd\",\n",
    "         \"hit\", \"hold\", \"hop_on\", \"hose\", \"hug\", \"hunt\", \"inspect\", \"install\", \"jump\", \"kick\", \"kiss\", \"lasso\", \n",
    "         \"launch\", \"lick\", \"lie_on\", \"lift\", \"light\", \"load\", \"lose\", \"make\", \"milk\", \"move\", \"no_interaction\",\n",
    "         \"open\", \"operate\", \"pack\", \"paint\", \"park\", \"pay\", \"peel\", \"pet\", \"pick\", \"pick_up\", \"point\", \"pour\",\n",
    "         \"pull\", \"push\", \"race\", \"read\", \"release\", \"repair\", \"ride\", \"row\", \"run\", \"sail\", \"scratch\", \"serve\",\n",
    "         \"set\", \"shear\", \"sign\", \"sip\", \"sit_at\", \"sit_on\", \"slide\", \"smell\", \"spin\", \"squeeze\", \"stab\", \"stand_on\",\n",
    "         \"stand_under\", \"stick\", \"stir\", \"stop_at\", \"straddle\", \"swing\", \"tag\", \"talk_on\", \"teach\", \"text_on\", \"throw\",\n",
    "         \"tie\", \"toast\", \"train\", \"turn\", \"type_on\", \"walk\", \"wash\", \"watch\", \"wave\", \"wear\", \"wield\", \"zip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "def hook(module, input, output):\n",
    "    outputs.append(output)\n",
    "faster_rcnn.roi_heads.box_head.fc7.register_forward_hook(hook)\n",
    "\n",
    "img_norm = img / 255\n",
    "frcnn_img_tensor = torch.from_numpy(img_norm)\n",
    "frcnn_img_tensor = frcnn_img_tensor.permute([2,0,1]).float().to(device) # chw format\n",
    "rcnn_input = [frcnn_img_tensor]\n",
    "\n",
    "out = faster_rcnn(rcnn_input)[0]\n",
    "features = outputs[0]\n",
    "node_num.append(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = None\n",
    "for i in range(len(out['scores'])):\n",
    "    if out['scores'][i] < 0.7:\n",
    "        bboxes = out['boxes'][:i].detach().cpu()\n",
    "        roi_labels = [out['labels'][:i].detach().cpu().numpy()]\n",
    "img_wh = [img.shape[1], img.shape[0]]\n",
    "spatial_feat = spatial.calculate_spatial_feats(bboxes, img_wh)\n",
    "spatial_feat = torch.Tensor(spatial_feat).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_emb = np.empty((0,300))\n",
    "for id in roi_labels[0]:\n",
    "    vec = word2vec[coco_dict[id]]\n",
    "    word2vec_emb = np.vstack((word2vec_emb, vec))\n",
    "word2vec_emb = torch.Tensor(word2vec_emb).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones_like(img) * 255\n",
    "for bbox in bboxes:\n",
    "    bbox = bbox.detach().cpu().numpy()\n",
    "    cv2.rectangle(mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 0, 0), thickness=-1)\n",
    "background_img = cv2.bitwise_and(img, mask, mask=None)\n",
    "background_img_resize = cv2.resize(background_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "background_img_tensor = torch.from_numpy(background_img)\n",
    "res_background_input = background_img_tensor.unsqueeze(0)\n",
    "res_background_input = res_background_input.permute([0,3,1,2]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f05b0ffe1997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnode_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroi_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnode_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_background_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    node_num = [len(roi_labels[0])]\n",
    "    features = features[:node_num[0]]\n",
    "    model_preds, nodes = model(node_num, features, spatial_feat, word2vec_emb, roi_labels, bg=res_background_input, validation=True)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 117])\n",
      "4\n",
      "[array([ 2,  1,  1, 27])]\n",
      "hold\n",
      "ride\n",
      "sit_on\n",
      "ride\n",
      "hold\n"
     ]
    }
   ],
   "source": [
    "model_preds = model_preds.detach().cpu()\n",
    "model_preds = torch.sigmoid(model_preds)\n",
    "preds = torch.Tensor()\n",
    "confs = torch.Tensor()\n",
    "\n",
    "print(model_preds.shape)\n",
    "print(node_num[0])\n",
    "print(roi_labels)\n",
    "\n",
    "for edge in model_preds:\n",
    "    conf, pred = torch.topk(edge, 5)\n",
    "    preds = torch.cat((preds, pred.float()))\n",
    "    confs = torch.cat((confs, conf.float()))\n",
    "\n",
    "return_predictions = {}\n",
    "_, top5_ids = torch.topk(confs, 5)\n",
    "\n",
    "for id in top5_ids:\n",
    "    print(verbs[int(preds[id.item()].item())])\n",
    "#     prediction_str = prediction['object'] + ' ' + prediction['verb']\n",
    "#     prediction_conf = round(top5_confidence[i].item() * 100, 2)\n",
    "#     return_predictions[prediction_str] = prediction_conf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (interact)",
   "language": "python",
   "name": "interact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
