{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ipdb\n",
    "import h5py\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import random\n",
    "\n",
    "import utils.io as io\n",
    "#from model.cnn_model import HOCNN\n",
    "from model.cnn_with_pose import HOPOSECNN\n",
    "from datasets import metadata\n",
    "from utils.vis_tool import vis_img\n",
    "from datasets.hico_constants import HicoConstants\n",
    "from datasets.hico_dataset import HicoDataset, collate_fn\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "TRAIN_IMG_PATH = \"datasets/hico/images/train2015/\"\n",
    "TRAIN_POSE_PATH = \"datasets/human_pose/train2015/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup training device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('training on {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arguments\n",
    "lr = 0.00001\n",
    "batch_size = 32\n",
    "epochs = 300\n",
    "\n",
    "feat_type = 'fc7'\n",
    "data_aug = False\n",
    "exp_ver = 'v1'\n",
    "save_dir = './checkpoints/hicopose'\n",
    "log_dir = './log/hicopose'\n",
    "save_every = 10\n",
    "print_batch_every = 250\n",
    "print_epoch_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fc7 feature...\n",
      "Using fc7 feature...\n",
      "set up dataset variable successfully\n",
      "set up dataloader successfully\n"
     ]
    }
   ],
   "source": [
    "# Define dataloaders\n",
    "data_const = HicoConstants(feat_type=feat_type)\n",
    "\n",
    "train_dataset = HicoDataset(data_const=data_const, subset='train', data_aug=data_aug)\n",
    "val_dataset = HicoDataset(data_const=data_const, subset='val', data_aug=False, test=True)\n",
    "dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "print('set up dataset variable successfully')\n",
    "\n",
    "train_dataloader = DataLoader(dataset=dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_dataloader = DataLoader(dataset=dataset['val'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "dataloader = {'train': train_dataloader, 'val': val_dataloader}\n",
    "print('set up dataloader successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = HOPOSECNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in this model is 50.857378 million\n"
     ]
    }
   ],
   "source": [
    "# Display parameter information\n",
    "parameter_num = 0\n",
    "for param in model.parameters():\n",
    "    parameter_num += param.numel()\n",
    "print(f'The number of parameters in this model is {parameter_num / 1e6} million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup visualization\n",
    "writer = SummaryWriter(log_dir=log_dir + '/' + exp_ver + '/' + 'epoch_train')\n",
    "io.mkdir_if_not_exists(os.path.join(save_dir, exp_ver, 'epoch_train'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyl02/.conda/envs/Interact/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "/home/jimmyl02/launchpad/interact/model/cnn_with_pose.py:212: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(results)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Epoch: 1/300 Batch: 250/952 Loss: 3.0035288333892822 \n",
      "[train] Epoch: 1/300 Batch: 500/952 Loss: 3.0572080612182617 \n",
      "[train] Epoch: 1/300 Batch: 750/952 Loss: 2.859103202819824 \n",
      "[train] Epoch: 1/300 Loss: 3.727549905397009 Execution time: 625.4824423789978\n",
      "[*] Writing to tensorboard\n",
      "[val] Epoch: 1/300 Loss: 3.059969277390029 Execution time: 156.54567432403564\n",
      "[train] Epoch: 2/300 Batch: 250/952 Loss: 2.637631893157959 \n",
      "[train] Epoch: 2/300 Batch: 500/952 Loss: 2.678720474243164 \n",
      "[train] Epoch: 2/300 Batch: 750/952 Loss: 4.238250732421875 \n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "with open('datasets/processed/hico/anno_list.json') as f:\n",
    "    anno_list = json.load(f)\n",
    "    \n",
    "print('Training has started!')\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for phase in ['train', 'val']:\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        idx = 0\n",
    "        \n",
    "        for data in dataloader[phase]:\n",
    "            train_data = data\n",
    "            img_name = train_data['img_name']\n",
    "            \n",
    "            labels = np.zeros((batch_size, 600))\n",
    "            for i in range(batch_size):\n",
    "                # Get image data\n",
    "                parsed_img_name = img_name[i].split(\".\")[0]\n",
    "                img = [x for x in anno_list if x['global_id'] == parsed_img_name][0]\n",
    "                img = img['hois'][0]\n",
    "                img_id = int(img['id']) - 1\n",
    "                labels[i][img_id] = 1\n",
    "                human_bboxes = img['human_bboxes']\n",
    "                object_bboxes = img['object_bboxes']\n",
    "\n",
    "                # Apply masks to images\n",
    "                src = cv2.imread(TRAIN_IMG_PATH + 'HICO_train2015_' + img['id'].rjust(8, '0') + '.jpg')\n",
    "                human_mask = np.zeros_like(src)\n",
    "                for bbox in human_bboxes:\n",
    "                    cv2.rectangle(human_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                human_bbox_img = cv2.bitwise_and(src, human_mask, mask=None)\n",
    "\n",
    "                obj_mask = np.zeros_like(src)\n",
    "                pairwise_mask = human_mask\n",
    "                for bbox in object_bboxes:\n",
    "                    cv2.rectangle(obj_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                    cv2.rectangle(pairwise_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                obj_bbox_img = cv2.bitwise_and(src, obj_mask, mask=None)\n",
    "                pairwise_bbox_img = cv2.bitwise_and(src, pairwise_mask, mask=None)\n",
    "\n",
    "                # Get pose data\n",
    "                with open(TRAIN_POSE_PATH + 'HICO_train2015_' + img['id'].rjust(8, '0') + '_keypoints.json') as f:\n",
    "                    pose_data = json.load(f)\n",
    "\n",
    "                pose_img = np.zeros((src.shape[0], src.shape[1], 1))\n",
    "\n",
    "                for person in pose_data['people']:\n",
    "                    pose_data_2d = person['pose_keypoints_2d']\n",
    "                    prev_point = None\n",
    "                    for inner_point in range(0, len(pose_data_2d), 3):\n",
    "                        x, y, c = pose_data_2d[inner_point], pose_data_2d[inner_point + 1], pose_data_2d[inner_point + 2]\n",
    "                        if c > 0:\n",
    "                            pose_img = cv2.circle(pose_img, (int(x), int(y)), 10, (255,0,0), \\\n",
    "                                                  thickness=-1, lineType=cv2.FILLED)\n",
    "                            if prev_point:\n",
    "                                pose_img = cv2.line(pose_img, (int(prev_point[0]), int(prev_point[1])), \\\n",
    "                                                    (int(x), int(y)), (255, 0, 0), 3)\n",
    "                            prev_point = (x, y)\n",
    "\n",
    "                '''\n",
    "                # Visualization of masks and pose\n",
    "                f, axarr = plt.subplots(1,4)\n",
    "\n",
    "                human_bbox_rgb = cv2.cvtColor(human_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[0].imshow(human_bbox_rgb)\n",
    "\n",
    "                object_mask_rgb = cv2.cvtColor(obj_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[1].imshow(object_mask_rgb)\n",
    "\n",
    "                pairwise_rgb = cv2.cvtColor(pairwise_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[2].imshow(pairwise_rgb)\n",
    "\n",
    "                zeroed_channel = np.zeros((src.shape[0], src.shape[1], 1))\n",
    "                stacked_pose_img = np.dstack((pose_img, zeroed_channel, zeroed_channel))\n",
    "                axarr[3].imshow(stacked_pose_img)\n",
    "\n",
    "                plt.show()\n",
    "                f.clf()\n",
    "                '''\n",
    "\n",
    "                human_bbox_img = cv2.resize(human_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "                obj_bbox_img = cv2.resize(obj_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "                pairwise_bbox_img = cv2.resize(pairwise_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "                pose_img = cv2.resize(pose_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                human_bbox_img = torch.from_numpy(human_bbox_img).to(device)\n",
    "                obj_bbox_img = torch.from_numpy(obj_bbox_img).to(device)\n",
    "                pairwise_bbox_img = torch.from_numpy(pairwise_bbox_img).to(device)\n",
    "                pose_img = torch.from_numpy(pose_img).to(device)\n",
    "\n",
    "                if i == 0:\n",
    "                    res_human_input = human_bbox_img.unsqueeze(0)\n",
    "                    res_obj_input = obj_bbox_img.unsqueeze(0)\n",
    "                    res_pairwise_input = pairwise_bbox_img.unsqueeze(0)\n",
    "                    res_pose_input = pose_img.unsqueeze(0)\n",
    "                else:\n",
    "                    res_human_input = torch.cat((res_human_input, human_bbox_img.unsqueeze(0)), dim=0)\n",
    "                    res_obj_input = torch.cat((res_obj_input, obj_bbox_img.unsqueeze(0)), dim=0)\n",
    "                    res_pairwise_input = torch.cat((res_pairwise_input, pairwise_bbox_img.unsqueeze(0)), dim=0)\n",
    "                    res_pose_input = torch.cat((res_pose_input, pose_img.unsqueeze(0)), dim=0)\n",
    "\n",
    "            res_human_input = res_human_input.permute([0,3,1,2]).float().to(device)\n",
    "            res_obj_input = res_obj_input.permute([0,3,1,2]).float().to(device)\n",
    "            res_pairwise_input = res_pairwise_input.permute([0,3,1,2]).float().to(device)\n",
    "            res_pose_input = res_pose_input.unsqueeze(3).permute([0,3,1,2]).float().to(device)\n",
    "            labels = torch.from_numpy(labels).long().to(device)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                # Initial train loop\n",
    "                model.train()\n",
    "                model.zero_grad()\n",
    "                \n",
    "                # Forward pass: human, objects, pairwise streams\n",
    "                outputs = model.forward(res_human_input, res_obj_input, res_pairwise_input, res_pose_input)\n",
    "                loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            else:\n",
    "                # Evaluation after train loop\n",
    "                model.eval()\n",
    "                with torch.no_grad(): # Disable gradients for validation\n",
    "                    outputs = model.forward(res_human_input, res_obj_input, res_pairwise_input, res_pose_input)\n",
    "                    loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "                    \n",
    "            # Accumulate loss of each batch (average * batch size)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            \n",
    "            # Print out status per print_batch_every\n",
    "            idx += 1\n",
    "            if (idx % print_batch_every) == 0:\n",
    "                print(\"[{}] Epoch: {}/{} Batch: {}/{} Loss: {} \".format(\\\n",
    "                        phase, epoch+1, epochs, idx, len(dataloader[phase]), loss.item()))\n",
    "            \n",
    "        # Epoch loss\n",
    "        epoch_loss = running_loss / len(dataset[phase])\n",
    "        \n",
    "        # Log trainval data for visualization\n",
    "        if phase == 'train':\n",
    "            train_loss = epoch_loss \n",
    "        else:\n",
    "            writer.add_scalars('trainval_loss_epoch', {'train': train_loss, 'val': epoch_loss}, epoch)\n",
    "            \n",
    "        # Output data per print_epoch_every\n",
    "        if (epoch % print_epoch_every) == 0:\n",
    "            end_time = time.time()\n",
    "            print(\"[{}] Epoch: {}/{} Loss: {} Execution time: {}\".format(\\\n",
    "                    phase, epoch+1, epochs, epoch_loss, (end_time-start_time)))\n",
    "            \n",
    "    # Save the model per save_every\n",
    "    if epoch_loss<0.0405 or epoch % save_every == (save_every - 1) and epoch >= (200-1):\n",
    "        checkpoint = { \n",
    "                        'lr': lr,\n",
    "                       'b_s': batch_size,\n",
    "                 'feat_type': feat_type,\n",
    "                'state_dict': model.state_dict()\n",
    "        }\n",
    "        save_name = \"checkpoint_\" + str(epoch+1) + '_epoch.pth'\n",
    "        torch.save(checkpoint, os.path.join(save_dir, exp_ver, 'epoch_train', save_name))\n",
    "        \n",
    "print('Finishing training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close visualization\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interact",
   "language": "python",
   "name": "interact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
