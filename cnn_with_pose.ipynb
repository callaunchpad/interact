{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ipdb\n",
    "import h5py\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import utils.io as io\n",
    "#from model.cnn_model import HOCNN\n",
    "from model.cnn_with_pose import HOPOSECNN\n",
    "from datasets import metadata\n",
    "from utils.vis_tool import vis_img\n",
    "from datasets.hico_constants import HicoConstants\n",
    "from datasets.hico_dataset import HicoDataset, collate_fn\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(21)\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "TRAIN_IMG_PATH = \"datasets/hico/images/train2015/\"\n",
    "TRAIN_POSE_PATH = \"datasets/human_pose/train2015/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('training on {}'.format(device))\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arguments\n",
    "batch_size = 32\n",
    "epochs = 80\n",
    "initial_lr = 0.00001\n",
    "\n",
    "feat_type = 'fc7'\n",
    "data_aug = False\n",
    "exp_ver = 'v3_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "save_dir = './checkpoints/hicopose'\n",
    "log_dir = './log/hicopose'\n",
    "save_every = 5\n",
    "print_batch_every = 100\n",
    "print_epoch_every = 1\n",
    "\n",
    "# set the cache size [0 means infinite]\n",
    "max_img_cache_size = 0#40000\n",
    "max_pose_cache_size = 0#40000\n",
    "\n",
    "print('Running experiment ' + exp_ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders\n",
    "data_const = HicoConstants(feat_type=feat_type)\n",
    "\n",
    "train_dataset = HicoDataset(data_const=data_const, subset='train', data_aug=data_aug)\n",
    "val_dataset = HicoDataset(data_const=data_const, subset='val', data_aug=False, test=True)\n",
    "dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "print('set up dataset variable successfully')\n",
    "\n",
    "train_dataloader = DataLoader(dataset=dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_dataloader = DataLoader(dataset=dataset['val'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "dataloader = {'train': train_dataloader, 'val': val_dataloader}\n",
    "print('set up dataloader successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = HOPOSECNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display parameter information\n",
    "parameter_num = 0\n",
    "for param in model.parameters():\n",
    "    parameter_num += param.numel()\n",
    "print(f'The number of parameters in this model is {parameter_num / 1e6} million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=0)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup visualization\n",
    "writer = SummaryWriter(log_dir=log_dir + '/' + exp_ver + '/' + 'epoch_train')\n",
    "io.mkdir_if_not_exists(os.path.join(save_dir, exp_ver, 'epoch_train'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "with open('datasets/processed/hico/anno_list.json') as f:\n",
    "    anno_list = json.load(f)\n",
    "    \n",
    "img_cache = {} # format {key: [human, object, pairwise]}\n",
    "img_cache_counter = 0\n",
    "pose_img_cache = {}\n",
    "pose_img_cache_counter = 0\n",
    "    \n",
    "print('Training has started!')\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for phase in ['train', 'val']:\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        idx = 0\n",
    "        \n",
    "        for data in dataloader[phase]:\n",
    "            train_data = data\n",
    "            img_name = train_data['img_name']\n",
    "            \n",
    "            labels = np.zeros((batch_size, 600))\n",
    "            batch_correct = 0\n",
    "            for i in range(batch_size):\n",
    "                # Get image data\n",
    "                parsed_img_name = img_name[i].split(\".\")[0]\n",
    "                img = [x for x in anno_list if x['global_id'] == parsed_img_name][0]\n",
    "                img = img['hois'][0]\n",
    "                img_id = int(img['id']) - 1\n",
    "                labels[i][img_id] = 1\n",
    "                human_bboxes = img['human_bboxes']\n",
    "                object_bboxes = img['object_bboxes']\n",
    "\n",
    "                # Apply masks to images [with caching]\n",
    "                src_img_path = TRAIN_IMG_PATH + 'HICO_train2015_' + img['id'].rjust(8, '0') + '.jpg'\n",
    "                if src_img_path in img_cache: # Use cache if available\n",
    "                    human_bbox_img, obj_bbox_img, pairwise_bbox_img = img_cache[src_img_path]\n",
    "                else:\n",
    "                    src = cv2.imread(src_img_path)\n",
    "                    human_mask = np.zeros_like(src)\n",
    "                    for bbox in human_bboxes:\n",
    "                        cv2.rectangle(human_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                    human_bbox_img = cv2.bitwise_and(src, human_mask, mask=None)\n",
    "\n",
    "                    obj_mask = np.zeros_like(src)\n",
    "                    pairwise_mask = human_mask\n",
    "                    for bbox in object_bboxes:\n",
    "                        cv2.rectangle(obj_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                        cv2.rectangle(pairwise_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                    obj_bbox_img = cv2.bitwise_and(src, obj_mask, mask=None)\n",
    "                    pairwise_bbox_img = cv2.bitwise_and(src, pairwise_mask, mask=None)\n",
    "                    \n",
    "                    # Add to cache if within limits\n",
    "                    if not max_img_cache_size or img_cache_counter < max_img_cache_size:\n",
    "                        img_cache[src_img_path] = [human_bbox_img, obj_bbox_img, pairwise_bbox_img]\n",
    "                        img_cache_counter += 1\n",
    "\n",
    "                # Get pose data [with caching]\n",
    "                pose_img_path = TRAIN_POSE_PATH + 'HICO_train2015_' + img['id'].rjust(8, '0') + '_keypoints.json'\n",
    "                if pose_img_path in pose_img_cache: # Use cache if available\n",
    "                    pose_img = pose_img_cache[pose_img_path]\n",
    "                else:\n",
    "                    with open(pose_img_path) as f:\n",
    "                        pose_data = json.load(f)\n",
    "\n",
    "                    pose_img = np.zeros((src.shape[0], src.shape[1], 1))\n",
    "\n",
    "                    for person in pose_data['people']:\n",
    "                        pose_data_2d = person['pose_keypoints_2d']\n",
    "                        prev_point = None\n",
    "                        for inner_point in range(0, len(pose_data_2d), 3):\n",
    "                            x, y, c = pose_data_2d[inner_point], pose_data_2d[inner_point + 1], pose_data_2d[inner_point + 2]\n",
    "                            if c > 0:\n",
    "                                pose_img = cv2.circle(pose_img, (int(x), int(y)), 10, (255,0,0), \\\n",
    "                                                      thickness=-1, lineType=cv2.FILLED)\n",
    "                                if prev_point:\n",
    "                                    pose_img = cv2.line(pose_img, (int(prev_point[0]), int(prev_point[1])), \\\n",
    "                                                        (int(x), int(y)), (255, 0, 0), 3)\n",
    "                                prev_point = (x, y)\n",
    "\n",
    "                    # Add to cache if within limits\n",
    "                    if not max_pose_cache_size or pose_img_cache_counter < max_pose_cache_size:\n",
    "                        pose_img_cache[pose_img_path] = pose_img\n",
    "                        pose_img_cache_counter += 1\n",
    "\n",
    "                '''\n",
    "                # Visualization of masks and pose\n",
    "                f, axarr = plt.subplots(1,4)\n",
    "\n",
    "                human_bbox_rgb = cv2.cvtColor(human_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[0].imshow(human_bbox_rgb)\n",
    "\n",
    "                object_mask_rgb = cv2.cvtColor(obj_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[1].imshow(object_mask_rgb)\n",
    "\n",
    "                pairwise_rgb = cv2.cvtColor(pairwise_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[2].imshow(pairwise_rgb)\n",
    "\n",
    "                zeroed_channel = np.zeros((src.shape[0], src.shape[1], 1))\n",
    "                stacked_pose_img = np.dstack((pose_img, zeroed_channel, zeroed_channel))\n",
    "                axarr[3].imshow(stacked_pose_img)\n",
    "\n",
    "                plt.show()\n",
    "                f.clf()\n",
    "                '''\n",
    "\n",
    "                human_bbox_img = cv2.resize(human_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "                obj_bbox_img = cv2.resize(obj_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "                pairwise_bbox_img = cv2.resize(pairwise_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "                pose_img = cv2.resize(pose_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                human_bbox_img = torch.from_numpy(human_bbox_img).to(device)\n",
    "                obj_bbox_img = torch.from_numpy(obj_bbox_img).to(device)\n",
    "                pairwise_bbox_img = torch.from_numpy(pairwise_bbox_img).to(device)\n",
    "                pose_img = torch.from_numpy(pose_img).to(device)\n",
    "\n",
    "                if i == 0:\n",
    "                    res_human_input = human_bbox_img.unsqueeze(0)\n",
    "                    res_obj_input = obj_bbox_img.unsqueeze(0)\n",
    "                    res_pairwise_input = pairwise_bbox_img.unsqueeze(0)\n",
    "                    res_pose_input = pose_img.unsqueeze(0)\n",
    "                else:\n",
    "                    res_human_input = torch.cat((res_human_input, human_bbox_img.unsqueeze(0)), dim=0)\n",
    "                    res_obj_input = torch.cat((res_obj_input, obj_bbox_img.unsqueeze(0)), dim=0)\n",
    "                    res_pairwise_input = torch.cat((res_pairwise_input, pairwise_bbox_img.unsqueeze(0)), dim=0)\n",
    "                    res_pose_input = torch.cat((res_pose_input, pose_img.unsqueeze(0)), dim=0)\n",
    "\n",
    "            res_human_input = res_human_input.permute([0,3,1,2]).float().to(device)\n",
    "            res_obj_input = res_obj_input.permute([0,3,1,2]).float().to(device)\n",
    "            res_pairwise_input = res_pairwise_input.permute([0,3,1,2]).float().to(device)\n",
    "            res_pose_input = res_pose_input.unsqueeze(3).permute([0,3,1,2]).float().to(device)\n",
    "            labels = torch.from_numpy(labels).long().to(device)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                # Initial train loop\n",
    "                model.train()\n",
    "                model.zero_grad()\n",
    "                \n",
    "                # Forward pass: human, objects, pairwise streams\n",
    "                outputs = model.forward(res_human_input, res_obj_input, res_pairwise_input, res_pose_input)\n",
    "                loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                ground_labels = torch.max(labels, 1)[1]\n",
    "                for accuracy_iterator in range(len(ground_labels)):\n",
    "                    if preds[accuracy_iterator] == ground_labels[accuracy_iterator]:\n",
    "                        batch_correct += 1\n",
    "                \n",
    "            else:\n",
    "                # Evaluation after train loop\n",
    "                model.eval()\n",
    "                with torch.no_grad(): # Disable gradients for validation\n",
    "                    outputs = model.forward(res_human_input, res_obj_input, res_pairwise_input, res_pose_input)\n",
    "                    loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "                    \n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    ground_labels = torch.max(labels, 1)[1]\n",
    "                    for accuracy_iterator in range(len(ground_labels)):\n",
    "                        if preds[accuracy_iterator] == ground_labels[accuracy_iterator]:\n",
    "                            batch_correct += 1\n",
    "                    \n",
    "            # Accumulate loss of each batch (average * batch size)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_correct += batch_correct\n",
    "            \n",
    "            # Print out status per print_batch_every\n",
    "            idx += 1\n",
    "            if (idx % print_batch_every) == 0:\n",
    "                print(\"[{}] Epoch: {}/{} Batch: {}/{} Loss: {} Accuracy: {}\".format(\\\n",
    "                        phase, epoch+1, epochs, idx, len(dataloader[phase]), \\\n",
    "                        loss.item(), 100 * batch_correct / batch_size))\n",
    "            \n",
    "        # Epoch loss and accuracy\n",
    "        epoch_loss = running_loss / len(dataset[phase])\n",
    "        epoch_accuracy = 100 * running_correct / len(dataset[phase])\n",
    "        \n",
    "        # Log trainval data for visualization\n",
    "        if phase == 'train':\n",
    "            train_loss = epoch_loss \n",
    "            train_accuracy = epoch_accuracy\n",
    "        else:\n",
    "            writer.add_scalars('trainval_loss_epoch', {'train': train_loss, 'val': epoch_loss}, epoch)\n",
    "            writer.add_scalars('trainval_accuracy_epoch', {'train': train_accuracy, 'val': epoch_accuracy}, epoch)\n",
    "            \n",
    "        # Output data per print_epoch_every\n",
    "        if (epoch % print_epoch_every) == 0:\n",
    "            end_time = time.time()\n",
    "            print(\"[{}] Epoch: {}/{} Loss: {} Accuracy: {} Execution time: {}\".format(\\\n",
    "                    phase, epoch+1, epochs, epoch_loss, epoch_accuracy, (end_time-start_time)))\n",
    "    \n",
    "    # Save the model per save_every\n",
    "    if epoch_loss<0.0405 or epoch % save_every == (save_every - 1) and epoch >= (10-1):\n",
    "        checkpoint = { \n",
    "                        'lr': initial_lr,\n",
    "                       'b_s': batch_size,\n",
    "                 'feat_type': feat_type,\n",
    "                'state_dict': model.state_dict()\n",
    "        }\n",
    "        save_name = \"checkpoint_\" + str(epoch+1) + '_epoch.pth'\n",
    "        torch.save(checkpoint, os.path.join(save_dir, exp_ver, 'epoch_train', save_name))\n",
    "        \n",
    "print('Finishing training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close visualization\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interact",
   "language": "python",
   "name": "interact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
