{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ipdb\n",
    "import h5py\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import utils.io as io\n",
    "#from model.cnn_model import HOCNN\n",
    "from model.cnn_with_pose import HOPOSECNN\n",
    "from datasets import metadata\n",
    "from utils.vis_tool import vis_img\n",
    "from datasets.hico_constants import HicoConstants\n",
    "from datasets.hico_dataset import HicoDataset, collate_fn\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(21)\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "TRAIN_IMG_PATH = \"datasets/hico/images/train2015/\"\n",
    "TRAIN_POSE_PATH = \"datasets/human_pose/train2015/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Setup training device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('training on {}'.format(device))\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment v3_2020-11-02_22-58\n"
     ]
    }
   ],
   "source": [
    "# Define arguments\n",
    "batch_size = 32\n",
    "epochs = 80\n",
    "initial_lr = 0.00001\n",
    "\n",
    "feat_type = 'fc7'\n",
    "data_aug = False\n",
    "exp_ver = 'v3_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "save_dir = './checkpoints/hicopose'\n",
    "log_dir = './log/hicopose'\n",
    "save_every = 5\n",
    "print_batch_every = 100\n",
    "print_epoch_every = 1\n",
    "\n",
    "# set the cache size [0 means infinite]\n",
    "max_img_cache_size = 0#40000\n",
    "max_pose_cache_size = 0#40000\n",
    "\n",
    "print('Running experiment ' + exp_ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fc7 feature...\n",
      "Using fc7 feature...\n",
      "set up dataset variable successfully\n",
      "set up dataloader successfully\n"
     ]
    }
   ],
   "source": [
    "# Define dataloaders\n",
    "data_const = HicoConstants(feat_type=feat_type)\n",
    "\n",
    "train_dataset = HicoDataset(data_const=data_const, subset='train', data_aug=data_aug)\n",
    "val_dataset = HicoDataset(data_const=data_const, subset='val', data_aug=False, test=True)\n",
    "dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "print('set up dataset variable successfully')\n",
    "\n",
    "train_dataloader = DataLoader(dataset=dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_dataloader = DataLoader(dataset=dataset['val'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "dataloader = {'train': train_dataloader, 'val': val_dataloader}\n",
    "print('set up dataloader successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = HOPOSECNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in this model is 50.857381 million\n"
     ]
    }
   ],
   "source": [
    "# Display parameter information\n",
    "parameter_num = 0\n",
    "for param in model.parameters():\n",
    "    parameter_num += param.numel()\n",
    "print(f'The number of parameters in this model is {parameter_num / 1e6} million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=0)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup visualization\n",
    "writer = SummaryWriter(log_dir=log_dir + '/' + exp_ver + '/' + 'epoch_train')\n",
    "io.mkdir_if_not_exists(os.path.join(save_dir, exp_ver, 'epoch_train'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyl02/.conda/envs/Interact/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "/home/jimmyl02/launchpad/interact/model/cnn_with_pose.py:215: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(summed_results)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Epoch: 1/80 Batch: 100/952 Loss: 2.524625062942505 Accuracy: 65.625\n",
      "[train] Epoch: 1/80 Batch: 200/952 Loss: 1.7821564674377441 Accuracy: 84.375\n",
      "[train] Epoch: 1/80 Batch: 300/952 Loss: 0.5612632036209106 Accuracy: 90.625\n",
      "[train] Epoch: 1/80 Batch: 400/952 Loss: 0.6929028630256653 Accuracy: 93.75\n",
      "[train] Epoch: 1/80 Batch: 500/952 Loss: 0.210893452167511 Accuracy: 96.875\n",
      "[train] Epoch: 1/80 Batch: 600/952 Loss: 0.06634172797203064 Accuracy: 100.0\n",
      "[train] Epoch: 1/80 Batch: 700/952 Loss: 0.7199876308441162 Accuracy: 90.625\n",
      "[train] Epoch: 1/80 Batch: 800/952 Loss: 0.21624018251895905 Accuracy: 96.875\n",
      "[train] Epoch: 1/80 Batch: 900/952 Loss: 0.7787693738937378 Accuracy: 90.625\n",
      "[train] Epoch: 1/80 Loss: 1.6048999865351397 Accuracy: 83.96130513198885 Execution time: 600.4731025695801\n",
      "[val] Epoch: 1/80 Batch: 100/238 Loss: 0.03511505946516991 Accuracy: 100.0\n",
      "[val] Epoch: 1/80 Batch: 200/238 Loss: 0.02162366732954979 Accuracy: 100.0\n",
      "[val] Epoch: 1/80 Loss: 0.15614109353515304 Accuracy: 98.084743539289 Execution time: 146.33405089378357\n",
      "[train] Epoch: 2/80 Batch: 100/952 Loss: 0.15814808011054993 Accuracy: 96.875\n",
      "[train] Epoch: 2/80 Batch: 200/952 Loss: 0.01121645886451006 Accuracy: 100.0\n",
      "[train] Epoch: 2/80 Batch: 300/952 Loss: 0.007562771439552307 Accuracy: 100.0\n",
      "[train] Epoch: 2/80 Batch: 400/952 Loss: 0.009764070622622967 Accuracy: 100.0\n",
      "[train] Epoch: 2/80 Batch: 500/952 Loss: 0.009238024242222309 Accuracy: 100.0\n",
      "[train] Epoch: 2/80 Batch: 600/952 Loss: 0.008232282474637032 Accuracy: 100.0\n",
      "[train] Epoch: 2/80 Batch: 700/952 Loss: 0.0036203437484800816 Accuracy: 100.0\n",
      "[train] Epoch: 2/80 Batch: 800/952 Loss: 0.2046806663274765 Accuracy: 96.875\n",
      "[train] Epoch: 2/80 Batch: 900/952 Loss: 0.015262088738381863 Accuracy: 100.0\n",
      "[train] Epoch: 2/80 Loss: 0.07650926642988447 Accuracy: 99.0359075258239 Execution time: 587.0286011695862\n",
      "[val] Epoch: 2/80 Batch: 100/238 Loss: 0.00917135365307331 Accuracy: 100.0\n",
      "[val] Epoch: 2/80 Batch: 200/238 Loss: 0.005372805520892143 Accuracy: 100.0\n",
      "[val] Epoch: 2/80 Loss: 0.07297043235701056 Accuracy: 99.23914469369015 Execution time: 141.86376857757568\n",
      "[train] Epoch: 3/80 Batch: 100/952 Loss: 0.0026641369331628084 Accuracy: 100.0\n",
      "[train] Epoch: 3/80 Batch: 200/952 Loss: 0.008244252763688564 Accuracy: 100.0\n",
      "[train] Epoch: 3/80 Batch: 300/952 Loss: 0.003057712921872735 Accuracy: 100.0\n",
      "[train] Epoch: 3/80 Batch: 400/952 Loss: 0.0038794218562543392 Accuracy: 100.0\n",
      "[train] Epoch: 3/80 Batch: 500/952 Loss: 0.0850648581981659 Accuracy: 96.875\n",
      "[train] Epoch: 3/80 Batch: 600/952 Loss: 0.0033206988591700792 Accuracy: 100.0\n",
      "[train] Epoch: 3/80 Batch: 700/952 Loss: 0.0026896544732153416 Accuracy: 100.0\n",
      "[train] Epoch: 3/80 Batch: 800/952 Loss: 0.3884270489215851 Accuracy: 96.875\n",
      "[train] Epoch: 3/80 Batch: 900/952 Loss: 0.0025364356115460396 Accuracy: 100.0\n",
      "[train] Epoch: 3/80 Loss: 0.02640868032390533 Accuracy: 99.58025905886211 Execution time: 574.7160778045654\n",
      "[val] Epoch: 3/80 Batch: 100/238 Loss: 0.001328155631199479 Accuracy: 100.0\n",
      "[val] Epoch: 3/80 Batch: 200/238 Loss: 0.004423522390425205 Accuracy: 100.0\n",
      "[val] Epoch: 3/80 Loss: 0.056911349862962514 Accuracy: 99.44903581267218 Execution time: 156.97018361091614\n",
      "[train] Epoch: 4/80 Batch: 100/952 Loss: 0.00219570635817945 Accuracy: 100.0\n",
      "[train] Epoch: 4/80 Batch: 200/952 Loss: 0.0020929365418851376 Accuracy: 100.0\n",
      "[train] Epoch: 4/80 Batch: 300/952 Loss: 0.0011728646932169795 Accuracy: 100.0\n",
      "[train] Epoch: 4/80 Batch: 400/952 Loss: 0.0007526017725467682 Accuracy: 100.0\n",
      "[train] Epoch: 4/80 Batch: 500/952 Loss: 0.0012386428425088525 Accuracy: 100.0\n",
      "[train] Epoch: 4/80 Batch: 600/952 Loss: 0.003012037603184581 Accuracy: 100.0\n",
      "[train] Epoch: 4/80 Batch: 700/952 Loss: 0.0010761278681457043 Accuracy: 100.0\n",
      "[train] Epoch: 4/80 Batch: 800/952 Loss: 0.0014850310981273651 Accuracy: 100.0\n",
      "[train] Epoch: 4/80 Batch: 900/952 Loss: 0.0005895923241041601 Accuracy: 100.0\n",
      "[train] Epoch: 4/80 Loss: 0.009751176971231823 Accuracy: 99.73766191178882 Execution time: 1029.4438562393188\n",
      "[val] Epoch: 4/80 Batch: 100/238 Loss: 0.0019403987098485231 Accuracy: 100.0\n",
      "[val] Epoch: 4/80 Batch: 200/238 Loss: 0.004036442842334509 Accuracy: 100.0\n",
      "[val] Epoch: 4/80 Loss: 0.055104563680759534 Accuracy: 99.54086317722681 Execution time: 176.30701994895935\n",
      "[train] Epoch: 5/80 Batch: 100/952 Loss: 0.011053874157369137 Accuracy: 100.0\n",
      "[train] Epoch: 5/80 Batch: 200/952 Loss: 0.0010729340137913823 Accuracy: 100.0\n",
      "[train] Epoch: 5/80 Batch: 300/952 Loss: 0.0002742019423749298 Accuracy: 100.0\n",
      "[train] Epoch: 5/80 Batch: 400/952 Loss: 0.0005406017298810184 Accuracy: 100.0\n",
      "[train] Epoch: 5/80 Batch: 500/952 Loss: 0.0003380169509910047 Accuracy: 100.0\n",
      "[train] Epoch: 5/80 Batch: 600/952 Loss: 0.0005358492489904165 Accuracy: 100.0\n",
      "[train] Epoch: 5/80 Batch: 700/952 Loss: 0.0010602734982967377 Accuracy: 100.0\n",
      "[train] Epoch: 5/80 Batch: 800/952 Loss: 0.0004450349952094257 Accuracy: 100.0\n",
      "[train] Epoch: 5/80 Batch: 900/952 Loss: 0.0003939855087082833 Accuracy: 100.0\n",
      "[train] Epoch: 5/80 Loss: 0.0027671997058983317 Accuracy: 99.84259714707329 Execution time: 593.0000598430634\n",
      "[val] Epoch: 5/80 Batch: 100/238 Loss: 0.00026256268029101193 Accuracy: 100.0\n",
      "[val] Epoch: 5/80 Batch: 200/238 Loss: 0.00013105463585816324 Accuracy: 100.0\n",
      "[val] Epoch: 5/80 Loss: 0.05578862835279155 Accuracy: 99.54086317722681 Execution time: 144.41804671287537\n",
      "[train] Epoch: 6/80 Batch: 100/952 Loss: 0.0001380971953039989 Accuracy: 100.0\n",
      "[train] Epoch: 6/80 Batch: 200/952 Loss: 0.0003281989193055779 Accuracy: 100.0\n",
      "[train] Epoch: 6/80 Batch: 300/952 Loss: 0.00017666391795501113 Accuracy: 100.0\n",
      "[train] Epoch: 6/80 Batch: 400/952 Loss: 0.00033318312489427626 Accuracy: 100.0\n",
      "[train] Epoch: 6/80 Batch: 500/952 Loss: 0.0028614411130547523 Accuracy: 100.0\n",
      "[train] Epoch: 6/80 Batch: 600/952 Loss: 0.04822269082069397 Accuracy: 96.875\n",
      "[train] Epoch: 6/80 Batch: 700/952 Loss: 0.00019828631775453687 Accuracy: 100.0\n",
      "[train] Epoch: 6/80 Batch: 800/952 Loss: 0.00016002175107132643 Accuracy: 100.0\n",
      "[train] Epoch: 6/80 Batch: 900/952 Loss: 0.00010147056309506297 Accuracy: 100.0\n",
      "[train] Epoch: 6/80 Loss: 0.0010502360334214002 Accuracy: 99.87211018199704 Execution time: 697.235246181488\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "with open('datasets/processed/hico/anno_list.json') as f:\n",
    "    anno_list = json.load(f)\n",
    "    \n",
    "img_cache = {} # format {key: [human, object, pairwise]}\n",
    "img_cache_counter = 0\n",
    "pose_img_cache = {}\n",
    "pose_img_cache_counter = 0\n",
    "    \n",
    "print('Training has started!')\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for phase in ['train', 'val']:\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        idx = 0\n",
    "        \n",
    "        for data in dataloader[phase]:\n",
    "            train_data = data\n",
    "            img_name = train_data['img_name']\n",
    "            \n",
    "            labels = np.zeros((batch_size, 600))\n",
    "            batch_correct = 0\n",
    "            for i in range(batch_size):\n",
    "                # Get image data\n",
    "                parsed_img_name = img_name[i].split(\".\")[0]\n",
    "                img = [x for x in anno_list if x['global_id'] == parsed_img_name][0]\n",
    "                img = img['hois'][0]\n",
    "                img_id = int(img['id']) - 1\n",
    "                labels[i][img_id] = 1\n",
    "                human_bboxes = img['human_bboxes']\n",
    "                object_bboxes = img['object_bboxes']\n",
    "\n",
    "                # Apply masks to images [with caching]\n",
    "                src_img_path = TRAIN_IMG_PATH + 'HICO_train2015_' + img['id'].rjust(8, '0') + '.jpg'\n",
    "                if src_img_path in img_cache: # Use cache if available\n",
    "                    human_bbox_img, obj_bbox_img, pairwise_bbox_img = img_cache[src_img_path]\n",
    "                else:\n",
    "                    src = cv2.imread(src_img_path)\n",
    "                    human_mask = np.zeros_like(src)\n",
    "                    for bbox in human_bboxes:\n",
    "                        cv2.rectangle(human_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                    human_bbox_img = cv2.bitwise_and(src, human_mask, mask=None)\n",
    "\n",
    "                    obj_mask = np.zeros_like(src)\n",
    "                    pairwise_mask = human_mask\n",
    "                    for bbox in object_bboxes:\n",
    "                        cv2.rectangle(obj_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                        cv2.rectangle(pairwise_mask, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 255, 255), thickness=-1)\n",
    "                    obj_bbox_img = cv2.bitwise_and(src, obj_mask, mask=None)\n",
    "                    pairwise_bbox_img = cv2.bitwise_and(src, pairwise_mask, mask=None)\n",
    "                    \n",
    "                    # Add to cache if within limits\n",
    "                    if not max_img_cache_size or img_cache_counter < max_img_cache_size:\n",
    "                        img_cache[src_img_path] = [human_bbox_img, obj_bbox_img, pairwise_bbox_img]\n",
    "                        img_cache_counter += 1\n",
    "\n",
    "                # Get pose data [with caching]\n",
    "                pose_img_path = TRAIN_POSE_PATH + 'HICO_train2015_' + img['id'].rjust(8, '0') + '_keypoints.json'\n",
    "                if pose_img_path in pose_img_cache: # Use cache if available\n",
    "                    pose_img = pose_img_cache[pose_img_path]\n",
    "                else:\n",
    "                    with open(pose_img_path) as f:\n",
    "                        pose_data = json.load(f)\n",
    "\n",
    "                    pose_img = np.zeros((src.shape[0], src.shape[1], 1))\n",
    "\n",
    "                    for person in pose_data['people']:\n",
    "                        pose_data_2d = person['pose_keypoints_2d']\n",
    "                        prev_point = None\n",
    "                        for inner_point in range(0, len(pose_data_2d), 3):\n",
    "                            x, y, c = pose_data_2d[inner_point], pose_data_2d[inner_point + 1], pose_data_2d[inner_point + 2]\n",
    "                            if c > 0:\n",
    "                                pose_img = cv2.circle(pose_img, (int(x), int(y)), 10, (255,0,0), \\\n",
    "                                                      thickness=-1, lineType=cv2.FILLED)\n",
    "                                if prev_point:\n",
    "                                    pose_img = cv2.line(pose_img, (int(prev_point[0]), int(prev_point[1])), \\\n",
    "                                                        (int(x), int(y)), (255, 0, 0), 3)\n",
    "                                prev_point = (x, y)\n",
    "\n",
    "                    # Add to cache if within limits\n",
    "                    if not max_pose_cache_size or pose_img_cache_counter < max_pose_cache_size:\n",
    "                        pose_img_cache[pose_img_path] = pose_img\n",
    "                        pose_img_cache_counter += 1\n",
    "\n",
    "                '''\n",
    "                # Visualization of masks and pose\n",
    "                f, axarr = plt.subplots(1,4)\n",
    "\n",
    "                human_bbox_rgb = cv2.cvtColor(human_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[0].imshow(human_bbox_rgb)\n",
    "\n",
    "                object_mask_rgb = cv2.cvtColor(obj_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[1].imshow(object_mask_rgb)\n",
    "\n",
    "                pairwise_rgb = cv2.cvtColor(pairwise_bbox_img, cv2.COLOR_BGR2RGB)\n",
    "                axarr[2].imshow(pairwise_rgb)\n",
    "\n",
    "                zeroed_channel = np.zeros((src.shape[0], src.shape[1], 1))\n",
    "                stacked_pose_img = np.dstack((pose_img, zeroed_channel, zeroed_channel))\n",
    "                axarr[3].imshow(stacked_pose_img)\n",
    "\n",
    "                plt.show()\n",
    "                f.clf()\n",
    "                '''\n",
    "\n",
    "                human_bbox_img = cv2.resize(human_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "                obj_bbox_img = cv2.resize(obj_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "                pairwise_bbox_img = cv2.resize(pairwise_bbox_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "                pose_img = cv2.resize(pose_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                human_bbox_img = torch.from_numpy(human_bbox_img).to(device)\n",
    "                obj_bbox_img = torch.from_numpy(obj_bbox_img).to(device)\n",
    "                pairwise_bbox_img = torch.from_numpy(pairwise_bbox_img).to(device)\n",
    "                pose_img = torch.from_numpy(pose_img).to(device)\n",
    "\n",
    "                if i == 0:\n",
    "                    res_human_input = human_bbox_img.unsqueeze(0)\n",
    "                    res_obj_input = obj_bbox_img.unsqueeze(0)\n",
    "                    res_pairwise_input = pairwise_bbox_img.unsqueeze(0)\n",
    "                    res_pose_input = pose_img.unsqueeze(0)\n",
    "                else:\n",
    "                    res_human_input = torch.cat((res_human_input, human_bbox_img.unsqueeze(0)), dim=0)\n",
    "                    res_obj_input = torch.cat((res_obj_input, obj_bbox_img.unsqueeze(0)), dim=0)\n",
    "                    res_pairwise_input = torch.cat((res_pairwise_input, pairwise_bbox_img.unsqueeze(0)), dim=0)\n",
    "                    res_pose_input = torch.cat((res_pose_input, pose_img.unsqueeze(0)), dim=0)\n",
    "\n",
    "            res_human_input = res_human_input.permute([0,3,1,2]).float().to(device)\n",
    "            res_obj_input = res_obj_input.permute([0,3,1,2]).float().to(device)\n",
    "            res_pairwise_input = res_pairwise_input.permute([0,3,1,2]).float().to(device)\n",
    "            res_pose_input = res_pose_input.unsqueeze(3).permute([0,3,1,2]).float().to(device)\n",
    "            labels = torch.from_numpy(labels).long().to(device)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                # Initial train loop\n",
    "                model.train()\n",
    "                model.zero_grad()\n",
    "                \n",
    "                # Forward pass: human, objects, pairwise streams\n",
    "                outputs = model.forward(res_human_input, res_obj_input, res_pairwise_input, res_pose_input)\n",
    "                loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                ground_labels = torch.max(labels, 1)[1]\n",
    "                for accuracy_iterator in range(len(ground_labels)):\n",
    "                    if preds[accuracy_iterator] == ground_labels[accuracy_iterator]:\n",
    "                        batch_correct += 1\n",
    "                \n",
    "            else:\n",
    "                # Evaluation after train loop\n",
    "                model.eval()\n",
    "                with torch.no_grad(): # Disable gradients for validation\n",
    "                    outputs = model.forward(res_human_input, res_obj_input, res_pairwise_input, res_pose_input)\n",
    "                    loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "                    \n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    ground_labels = torch.max(labels, 1)[1]\n",
    "                    for accuracy_iterator in range(len(ground_labels)):\n",
    "                        if preds[accuracy_iterator] == ground_labels[accuracy_iterator]:\n",
    "                            batch_correct += 1\n",
    "                    \n",
    "            # Accumulate loss of each batch (average * batch size)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_correct += batch_correct\n",
    "            \n",
    "            # Print out status per print_batch_every\n",
    "            idx += 1\n",
    "            if (idx % print_batch_every) == 0:\n",
    "                print(\"[{}] Epoch: {}/{} Batch: {}/{} Loss: {} Accuracy: {}\".format(\\\n",
    "                        phase, epoch+1, epochs, idx, len(dataloader[phase]), \\\n",
    "                        loss.item(), 100 * batch_correct / batch_size))\n",
    "            \n",
    "        # Epoch loss and accuracy\n",
    "        epoch_loss = running_loss / len(dataset[phase])\n",
    "        epoch_accuracy = 100 * running_correct / len(dataset[phase])\n",
    "        \n",
    "        # Log trainval data for visualization\n",
    "        if phase == 'train':\n",
    "            train_loss = epoch_loss \n",
    "            train_accuracy = epoch_accuracy\n",
    "        else:\n",
    "            writer.add_scalars('trainval_loss_epoch', {'train': train_loss, 'val': epoch_loss}, epoch)\n",
    "            writer.add_scalars('trainval_accuracy_epoch', {'train': train_accuracy, 'val': epoch_accuracy}, epoch)\n",
    "            \n",
    "        # Output data per print_epoch_every\n",
    "        if (epoch % print_epoch_every) == 0:\n",
    "            end_time = time.time()\n",
    "            print(\"[{}] Epoch: {}/{} Loss: {} Accuracy: {} Execution time: {}\".format(\\\n",
    "                    phase, epoch+1, epochs, epoch_loss, epoch_accuracy, (end_time-start_time)))\n",
    "    \n",
    "    # Save the model per save_every\n",
    "    if epoch_loss<0.0405 or epoch % save_every == (save_every - 1) and epoch >= (10-1):\n",
    "        checkpoint = { \n",
    "                        'lr': initial_lr,\n",
    "                       'b_s': batch_size,\n",
    "                 'feat_type': feat_type,\n",
    "                'state_dict': model.state_dict()\n",
    "        }\n",
    "        save_name = \"checkpoint_\" + str(epoch+1) + '_epoch.pth'\n",
    "        torch.save(checkpoint, os.path.join(save_dir, exp_ver, 'epoch_train', save_name))\n",
    "        \n",
    "print('Finishing training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close visualization\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interact",
   "language": "python",
   "name": "interact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
